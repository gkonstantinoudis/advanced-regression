---
title: "Advanced Regression: A note on collinearity"
date: "20-02-2023"
author:
  name: "Garyfallos Konstantinoudis"
institute: "Imperial College London"
editor: visual
date-format: medium
title-slide-attributes:
  data-background-color: "#f3f4f4"
  data-background-image: "../../assets/logo_trans.png"
  data-background-size: 50%
  data-background-position: 90% 120%
format:
    revealjs:
      theme: [default, ../../assets/style.scss]
      logo: "../../assets/logo_trans.png"
      slide-number: true
      incremental: false
      chalkboard:
        buttons: false
---

::: {style="font-size: 80%;"}
## Introduction

We consider again the diabetes outcome looking at the outcome disease progression $y$ and we try to fit the following linear model:

$y = \alpha + age + male + female + map + ltg$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"

library(lars)
library(dplyr)
library(corrplot)

data(diabetes)
x <- as.data.frame.matrix(diabetes$x)
y <- diabetes$y

x$male <- ifelse(x$sex < 0, 1, 0)
x$female <- ifelse(x$sex > 0, 1, 0)

corrplot(corr = x %>% dplyr::select(age, male, female, map, ltg) %>% as.matrix() %>% cor())
```
:::

::: {style="font-size: 80%;"}
## Fitting an lm when high correlation

```{r}
#| echo: true
#| eval: true

lm(y ~ age + male + female + map + ltg, data = x) %>% summary()
```

-   Option in `lm()` function: `singular.ok = TRUE` automatically removes female.

```{r}
#| echo: true
#| eval: true
#| error: true

lm(y ~ age + male + female + map + ltg, data = x, singular.ok = FALSE)
```
:::


::: {style="font-size: 80%;"}
## Fitting an lm when high correlation

-   The `lm()` function checks for singularities in the design matrix $x$, but not all methods have this safety check.

<!-- -->

-   Example: Lasso regression

```{r}
#| echo: true
#| eval: true
#| error: true
library(glmnet)
lm.lasso <- glmnet(y = y, x = x, alpha = 1, lambda = 0.5, family = "gaussian")
lm.lasso$beta
```
:::

::: {style="font-size: 80%;"}
## What are singularity and multicollinearity?

**Singularity**

One predictor variable in a multiple regression model can be exactly explained by the other $p-1$ predictor variables.

**Multicollinearity**

One predictor variable in a multiple regression model can be linearly explained by the other $p-1$ predictor variables with high accuracy.

What can cause singularity?

-   Dummy-coding of categorical variable. Make sure not to add redundant information

-   Do not include multiple measurement that are measured on different scales (e.g., $m$ and $cm$)
:::

::: {style="font-size: 80%;"}
## What is the impact of multicollinearity?

True biological processes do not cause singularity (because they are random, not deterministic), but can cause multicollinearity.

-   The computation of the ordinary least squares estimate requires an inversion of the $p \times p$-dimensional correlation matrix $x^tx$.

-   $x^tx$ cannot be inverted when $x^tx$ is singular.

-   When there is multicollinearity, $x^tx$ can be inverted, but the estimate will show a high variance and will be highly unstable.

-   Multicollinearity can distort a linear model and impact the interpretation.
:::

## How to inspect correlation?

For generic correlation structures:

-   Correlation and covariance matrix

How to detect singularity?

-   Rank of matrix

How to detect multicollinearity

-   Variance inflation factor


::: {style="font-size: 80%;"}
## Covariance matrix

Computing the sample covariance matrix using matrix multiplication

$$\hat{cov}(x) = \frac{1}{n-1} \underbrace{x^t_c}_{p \times n} \underbrace{x_c}_{n \times p}$$

-   $x_c$ is centred (mean is zero) $x_c = x- 1_n \bar{x} = cx$

    -   where $\bar{x}=(\bar{x}_1, …, \bar{x}_p)$ is the vector of means

    -   and $1_n$ is a vector of ones

    -   and $c=I_n - \frac{1}{n}1_n 1_n^t$

    -   and $I_n$ is the $n \times n$ identity matrix with ones on the diagonal

-   $x$ predictor matrix of $n$ rows and $p$ columns

-   $x^t$ transposed predictor matrix of $p$ rows and $n$ columns
:::

::: {style="font-size: 80%;"}
## Matrix multiplication

Matrix multiplication: $\underbrace{c}_{n \times p} = \underbrace{a}_{n \times m} \underbrace{b}_{m \times p}$

$$c_{ij} =  \sum_{k=1}^m a_{ik}b_{kj}$$

-   $a$ is a $n \times m$ and $b$ is a $m \times p$ matrix

-   $c$ is a $n \times m \times m \times p = n \times p$ matrix

-   Make sure your matrices have the correct dimensions, number of columns of the left matrix must be equal to the number of rows on the right.

-   Can be computed in R using the `\%*\%` command.
:::

::: {style="font-size: 80%;"}
## Correlation matrix

Computing the sample correlation matrix using matrix multiplication

$$\hat{cor}(x) = \frac{1}{n-1} \underbrace{x^t_s}_{p \times n} \underbrace{x_s}_{n \times p} \nonumber$$

where $x_s$ is a centred and scaled matrix $x_s=cxd^{-1}$

-   where \$d=diag(s)\$ is a diagonal matrix

-   with the sample standard deviation \$s\$ on the diagonal.

This is equivalent to writing

$$\hat{cor}(x_j, x_k) = \frac{\sum_{i=1}^n (x_{ij}-\bar{x}_j)(x_{ik} - \bar{x}_k) }{ \sqrt{\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2 } \sqrt{\sum_{i=1}^n (x_{ik} - \bar{x}_k)^2 }} $$
:::

::: {style="font-size: 80%;"}
## Correlation matrix

-   Correlation matrices are symmetric and have a vector of $1$'s on the diagonal.

```{r}
#| echo: true
#| eval: true
cor(x %>% dplyr::select(age, male, female, map, ltg))
```

-   Note the following correlation matrix captures the correlation between the samples and is of dimension $n \times n$

$$\hat{cor}(x^t) = \frac{1}{p-1} \underbrace{x_s}_{n \times p} \underbrace{x_s^t}_{p \times n}$$
:::

::: {style="font-size: 80%;"}
## Correlation matrix

R commands

-   `cov()` sample covariance matrix

-   `cor()` sample correlation matrix

-   `corrplot()` to visualise

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
corrplot(corr = x %>% dplyr::select(age, male, female, map, ltg) %>% as.matrix() %>% cor())
```
:::

## The rank of a matrix

-   Consider a matrix $x$ of dimension $n \times p$.

    $$\underbrace{x}_{n \times p} $$

-   The rank of matrix $x$ is the minimum of $n$ and $p$.

-   If we have more samples than variables ($n>p$) the rank is $p$.

If we have less samples than variables ($n<p$) the rank is $n$.

::: {style="font-size: 80%;"}
## The rank of the correlation matrix

-   Let us consider again the correlation matrix

$$\hat{cor}(x) = \frac{1}{n-1} \underbrace{x^t_s}_{p \times n} \underbrace{x_s}_{n \times p}$$

-   The theoretical rank of the correlation matrix is the minimum of $n$ and $p$.

-   To test the rank of a matrix in R: `rankMatrix()` in the `Matrix` package

If the rank of a correlation matrix is smaller than $min(n,p)$ the correlation matrix is singular and thus cannot be inverted.
:::

::: {style="font-size: 80%;"}
## The rank of the correlation matrix in R

```{r}
#| echo: true
#| eval: true
x %>%
  dplyr::select(age, male, female, map, ltg) %>%
  as.matrix() %>%
  cor()
x %>%
  dplyr::select(age, male, female, map, ltg) %>%
  as.matrix() %>%
  cor() %>%
  rankMatrix()
```

Interpretation: The correlation matrix of the design matrix with 5 predictors is of dimension $5\times5$, yet the rank is 4 which indicates singularity.
:::

## Variance ination factor (VIF)

-   The VIF is the ratio of the variance of $\beta_j$ when fitting the full model divided by the variance of $\beta_{UNI}(j)$ in a unvariable linear model.

-   Lowest possible value is 1 (no collinearity).

-   Rule of thumb: If VIF $> 10$, this indicates strong multicollinearity, but already smaller VIF can impact the analysis.

-   It provides an indication how much the variance of an estimated regression coefficient is increased because of multicollinearity.

::: {style="font-size: 70%;"}
## Variance inflation factor (VIF)

Consider the following linear model including $p$ predictors with inde $j \in 1,…,p$

$$y =  \alpha + \beta_1 x_1 + \beta_2 x_2 + … + \beta_j x_j + … + \beta_p x_p + \epsilon.$$

-   For the first variable $j=1$ fit a linear model, where $x_1$ is the outcome and all other variables $x_{-1}$ are the predictors

$$x_1 = \alpha +  \beta_2 x_2 + … + \beta_j x_j + … + \beta_p x_p + \epsilon.$$

-   Estimate $R_2(1)$, the proportion of variance of $x_1$ explained by the other predictors $x_{-1}$.

-   The VIF for variable $1$ is defined as

    $$VIF_1 =   \frac{1}{1-R_2(1)}$$

-   Repeat for the other $j \in 2,…,p$.
:::

## Variance inflation factor

R commands

-   `vif()` in the R-package `car`

-   Computes variance-inflation and generalized variance-inflation factors for linear and generalized linear models.

```{r}
#| echo: true
#| eval: true
library(car)
lm2 <- lm(y ~ age + male + map + ltg, data = x)
vif(lm2)
```

-   Interpretation: No variable has a VIF $>10$, with around $1$ they are rather low and there is no indication of multicollinearity.

## Summary

-   What are singularity and multicollinearity?

-   How to detect singularity and multicollinearity?

    -   Correlation and covariance matrix

    -   The rank of a matrix

    -   Variance in ation factor

Questions?
