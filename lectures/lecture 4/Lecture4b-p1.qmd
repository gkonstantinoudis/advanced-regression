---
title: "Advanced Regression 4b: Machine learning, decision trees"
date: "03-20-2026"
author:
  name: "Garyfallos Konstantinoudis"
institute: "Imperial College London"
editor: visual
date-format: medium
title-slide-attributes:
  data-background-color: "#f3f4f4"
  data-background-image: "../../assets/logo_trans.png"
  data-background-size: 50%
  data-background-position: 90% 120%
format:
    revealjs:
      theme: [default, ../../assets/style.scss]
      logo: "../../assets/logo_trans.png"
      slide-number: true
      incremental: false
      chalkboard:
        buttons: false
---

## 

-   Motivation for decision trees
-   Technical definition
-   Decision trees in `R`

## Decision trees and ensemble methods

-   **Decision tree**: A single tree
-   **Bagging**: A meta-algorithm over trees
-   **Random forest**: A meta-algorithm over random trees
-   **Boosting**: A meta-algorithm over sequential trees

```{r load-packages-and-data}
#| echo: false
#| message: false
#| warning: false

# dependency issue between ggplot and patchwork
# remotes::install_github("thomasp85/patchwork")
# remotes::install_github("tidyverse/ggplot2", ref = remotes::github_pull("5592"))

# partree
# install.packages("remotes")
# remotes::install_github("grantmcdermott/parttree")

library(tidyverse)
library(rpart)
library(rpart.plot)
library(parttree)
library(ggparty)
library(latex2exp)
library(patchwork)
library(palmerpenguins)
library(tree)
```

## Decision trees: an introduction

```{r basic-tree-1}
#| echo: false
#| eval: true
#| fig-align: "center"

df <- tibble(
  x = seq(from = 0, to = 2 * pi, length = 500),
  y = sin(x) + rnorm(length(x), sd = 0.5),
  truth = sin(x)
)

# run decision stump model
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 2)
fit <- rpart(y ~ x, data = df, control = ctrl)

py2 <- as.party(fit)

ggparty(py2) +
  geom_edge(size = 0.6, colour = "grey60") +
  geom_edge_label(aes(label = prettyNum(breaks_label, digits = 1)), size = 4) +
  geom_node_label(aes(label = splitvar), size = 4, ids = "inner") +
  geom_node_label(aes(label = str_c("Node ", id)),
    ids = "terminal",
    size = 4
  )
```

## Decision trees: an introduction

-   Decision trees are drawn upside down.

![](assets/upside-down-tree.png){width="80%," fig-align="center"}

## Decision trees: an introduction

Notation:

-   **Nodes** or **splits**: Points along the tree where the predictor space is split.
-   **Leaves**: Terminal nodes
-   **Branch**: Segments of a tree that connect the nodes

Outcomes:

-   **Quantitative**: Regression trees
-   **Categorical**: Classification trees considering $k = K$ categories

## Decision trees: an introduction

```{r basic-tree-2}
#| echo: false
#| eval: true
#| fig-align: "center"

data("WeatherPlay", package = "partykit")

df <- WeatherPlay |> rename(`Internal node` = outlook)
df <- df[1:5, 1:2]

pn <- partynode(1L, split = partysplit(1L, index = 1:2), kids = list(
  partynode(2L, split = partysplit(1L, index = 1:2), kids = list(
    partynode(3L, info = "leaf node"),
    partynode(4L, info = "leaf node")
  )),
  partynode(5L, info = "leaf node")
))

py <- party(pn, df)

ggparty(py) +
  geom_edge() +
  geom_node_label(aes(label = splitvar, col = factor(level)), ids = "inner") +
  geom_node_label(aes(label = info, col = factor(level)), ids = "terminal") +
  theme(legend.position = "none")
```

## Decision trees: another example

```{r tree-1d}
#| echo: false
#| eval: true
#| fig-align: "center"
df <- tibble(
  x = seq(from = 0, to = 2 * pi, length = 500),
  y = sin(x) + rnorm(length(x), sd = 0.5),
  truth = sin(x)
)

ctrl <- list(cp = 0, minbucket = 5, maxdepth = 1)
fit <- rpart(y ~ x, data = df, control = ctrl)

df |>
  mutate(pred = predict(fit, df)) |>
  ggplot(aes(x, y)) +
  geom_point(alpha = 0.2, size = 1) +
  geom_line(aes(x, y = truth), color = "grey25", linewidth = .75) +
  geom_line(aes(y = pred), colour = "darkcyan", linewidth = .75) +
  theme_minimal()
```

Problem: How to select the partition?

## How to fit a decision tree?

::: {style="font-size: 85%;"}
1.  Divide the predictor space ($x_1, x_2,..., x_p$) into $J$ distinct and non-overlapping regions, $r_1,r_m, ..., r_M$, where $m \in 1,...,M$.
2.  For every observation that falls in the same region $r_m$ we make the same prediction based on the mean (median) of all observations in region $r_m$.
3.  Define regions $r_1,r_2, ..., r_M$ to minimise the residual sum of squares $$
    RSS =  \sum_{m =1 }^M \sum_{i \in m} (y_i-\bar{y}_m)^2 \nonumber
    $$

-   Algorithm: Recursive binary splitting
:::

## Exercise: Reconstruct the tree

:::::: columns
::: {.column width="60%"}
```{r decision-boundary}
#| echo: false
#| eval: true
#| fig-align: "left"
p_tree <- ggplot() +
  annotate("text", x = 0, y = 0, label = "f", size = 0) +
  annotate("rect", xmin = -Inf, xmax = 0.2, ymin = 0.5, ymax = Inf, alpha = .75, colour = "grey10", fill = "grey95") +
  annotate("text", x = 0.1, y = 0.25, label = "R1", size = 10) +
  annotate("rect", xmin = -Inf, xmax = 0.2, ymin = 0.5, ymax = -Inf, alpha = .75, colour = "grey10", fill = "grey95") +
  annotate("text", x = 0.1, y = 0.75, label = "R2", size = 10) +
  annotate("rect", xmin = 0.2, xmax = 0.6, ymin = Inf, ymax = -Inf, alpha = .75, colour = "grey10", fill = "grey95") +
  annotate("text", x = 0.4, y = 0.5, label = "R3", size = 10) +
  annotate("rect", xmin = 0.6, xmax = Inf, ymin = 0.6, ymax = Inf, alpha = .75, colour = "grey10", fill = "grey95") +
  annotate("text", x = 0.8, y = 0.3, label = "R4", size = 10) +
  annotate("rect", xmin = 0.6, xmax = Inf, ymin = 0.6, ymax = -Inf, alpha = .75, colour = "grey10", fill = "grey95") +
  annotate("text", x = 0.8, y = 0.8, label = "R5", size = 10) +
  annotate("text", x = 1, y = 1, label = "f", size = 0) +
  annotate("text", x = 1, y = 0.6, label = TeX(r"($t_4)"), size = 8) +
  scale_x_continuous(
    breaks = c(0.2, 0.6),
    labels = c(TeX(r"($t_1)"), TeX(r"($t_2)"))
  ) +
  theme_minimal() +
  scale_y_continuous(
    breaks = c(0.5),
    labels = c(TeX(r"($t_3)"))
  ) +
  labs(x = TeX(r"($X_1)"), y = TeX(r"($X_2)")) +
  theme(
    axis.text = element_text(size = 18),
    axis.ticks = element_blank(),
    axis.title = element_text(size = 20)
  )
p_tree
```
:::

:::: {.column width="40%"}
::: {style="font-size: 80%;"}
-   Assume we have two variables, $X_1$ on the x-axis and $X_2$ on the y-axis.
-   $R_1$ to $R_5$ map out a partition.
-   $t_1$ to $t_4$ are the split values.
:::
::::
::::::

Reconstruct the respective tree

## Exercise: Reconstruct the tree

```{r tree-reconstruct}
#| echo: false
#| eval: true
#| fig-align: "center"
df <- WeatherPlay |> mutate(
  `X1 < t1` = outlook,
  `X2 < t3` = outlook,
  `X1 < t2` = outlook,
  `X2 < t4` = outlook
)

df <- df[, 6:9]


pn <- partynode(1L, split = partysplit(1L, index = 1:2), kids = list(
  partynode(2L, split = partysplit(2L, index = 1:2), kids = list(
    partynode(3L, info = "R1"),
    partynode(4L, info = "R2")
  )),
  partynode(5L, partysplit(3L, index = 1:2), info = "X1 < t4", kids = list(
    partynode(6L, info = "R3"),
    partynode(7L, partysplit(4L, index = 1:2), info = "X2 < t4", kids = list(
      partynode(8L, info = "R4"),
      partynode(9L, info = "R5")
    ))
  ))
))

py <- party(pn, df)

p <- ggparty(py) +
  geom_edge() +
  geom_node_label(aes(label = splitvar), ids = "inner", size = 8) +
  geom_node_label(aes(label = info, col = factor(level)), ids = "terminal", size = 6) +
  theme(legend.position = "none")

p_tree + p
```

## Implementation

::: {style="font-size: 80%;"}
For each variable $x_k$:

-   Find the optimal cutoff point $t$ :
    -   $$ \text{min}_s \text{MSE}(y_i |x_{ik} < t) + \text{MSE}(y_i |x_{ik} \geq t) $$
-   Choose variable yielding lowest MSE
-   Stop when MSE gain is too small
:::

```{r tree-reconstructed}
#| echo: false
#| eval: true
#| fig-align: "center"
p
```

## Example 1

```{r tree-example-1-data}
#| echo: false
#| eval: true

data_sim <- tibble(
  x = c(runif(10, 0, 0.2), runif(10, 0.2, 0.6), runif(10, 0.6, 0.8), runif(10, 0.8, 1.0)),
  y = c(rnorm(10, 0.2, 0.05), rnorm(10, 0.6, 0.05), rnorm(10, 0.8, 0.05), rnorm(10, 0.2, 0.05))
)

p_data <- ggplot() +
  geom_point(aes(x = x, y = y), data = data_sim) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_text(size = 20)
  )

p_data <- p_data +
  geom_vline(aes(xintercept = 0.6), colour = "red") +
  annotate("text", x = 0.6, y = 0.01, label = TeX(r"($s_1)"), size = 10)

p <- p_data +
  geom_segment(aes(x = 0.0, y = 0.4, xend = 0.6, yend = 0.4), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.01, y = 0.4, label = TeX(r"($y_1)"), size = 10) +
  geom_segment(aes(x = 0.6, y = 0.3, xend = 1.0, yend = 0.3), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.99, y = 0.3, label = TeX(r"($y_2)"), size = 10)


df <- WeatherPlay |> mutate(
  `X1 < s1` = outlook,
  `X2 < s2` = outlook,
  `X1 < s3` = outlook,
)

df <- df[, 6:8]

pn <- partynode(1L, split = partysplit(1L, index = 1:2), kids = list(
  partynode(2L, info = "y1"),
  partynode(3L, info = "y2")
))

py <- party(pn, df)

p_tree <- ggparty(py) +
  geom_edge() +
  geom_node_label(aes(label = splitvar), ids = "inner", size = 4) +
  geom_node_label(aes(label = info, col = factor(level)), ids = "terminal", size = 4) +
  theme(legend.position = "none")

p+p_tree
```

## Example 1

```{r tree-example-1-2}
#| echo: false
#| eval: true
#| fig-align: "left"

p_data <- p_data +
  geom_vline(aes(xintercept = 0.2), colour = "blue") +
  annotate("text", x = 0.2, y = 0.01, label = TeX(r"($s_2)"), size = 10)

p <- p_data +
  geom_segment(aes(x = 0.0, y = 0.2, xend = 0.2, yend = 0.2), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.01, y = 0.2, label = TeX(r"($y_1)"), size = 10) +
  geom_segment(aes(x = 0.2, y = 0.6, xend = 0.6, yend = 0.6), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.21, y = 0.6, label = TeX(r"($y_2)"), size = 10) +
  geom_segment(aes(x = 0.6, y = 0.3, xend = 1.0, yend = 0.3), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.99, y = 0.3, label = TeX(r"($y_3)"), size = 10)

pn <- partynode(1L, split = partysplit(1L, index = 1:2), kids = list(
  partynode(2L, split = partysplit(2L, index = 1:2), kids = list(
    partynode(3L, info = "y1"),
    partynode(4L, info = "y2")
  )),
  partynode(5L, info = "y3")
))

py <- party(pn, df)

p_tree <- ggparty(py) +
  geom_edge() +
  geom_node_label(aes(label = splitvar), ids = "inner", size = 4) +
  geom_node_label(aes(label = info, col = factor(level)), ids = "terminal", size = 5) +
  theme(legend.position = "none")

p + p_tree
```

## Example 1

```{r tree-example-1-3}
#| echo: false
#| eval: true
#| fig-align: "left"

p_data <- p_data +
  geom_vline(aes(xintercept = 0.8), colour = "green") +
  annotate("text", x = 0.8, y = 0.01, label = TeX(r"($s_3)"), size = 10)

p <- p_data +
  geom_segment(aes(x = 0.0, y = 0.2, xend = 0.2, yend = 0.2), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.01, y = 0.2, label = TeX(r"($y_1)"), size = 10) +
  geom_segment(aes(x = 0.2, y = 0.6, xend = 0.6, yend = 0.6), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.21, y = 0.6, label = TeX(r"($y_2)"), size = 10) +
  geom_segment(aes(x = 0.6, y = 0.8, xend = 0.8, yend = 0.8), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.61, y = 0.8, label = TeX(r"($y_3)"), size = 10) +
  geom_segment(aes(x = 0.8, y = 0.2, xend = 1.0, yend = 0.2), colour = "grey25", linetype = "dashed") +
  annotate("text", x = 0.81, y = 0.2, label = TeX(r"($y_4)"), size = 10)

pn <- partynode(1L, split = partysplit(1L, index = 1:2), kids = list(
  partynode(2L, split = partysplit(2L, index = 1:2), kids = list(
    partynode(3L, info = "y1"),
    partynode(4L, info = "y2")
  )),
  partynode(5L, partysplit(3L, index = 1:2), info = "X1 < t4", kids = list(
    partynode(6L, info = "y3"),
    partynode(7L, info = "y4")
  ))
))

py <- party(pn, df)

p_tree <- ggparty(py) +
  geom_edge() +
  geom_node_label(aes(label = splitvar), ids = "inner", size = 4) +
  geom_node_label(aes(label = info, col = factor(level)), ids = "terminal", size = 5) +
  theme(legend.position = "none")

p + p_tree
```

## Example 2

```{r tree-example-2}
#| echo: false
#| eval: true
#| fig-align: "left"

data("penguins", package = "palmerpenguins")

tree2 <- rpart(body_mass_g ~ flipper_length_mm + bill_length_mm, data = penguins)

py2 <- as.party(tree2)

p_tree <- ggparty(py2) +
  geom_edge(size = 0.6, colour = "grey60") +
  geom_edge_label(aes(label = prettyNum(breaks_label, digits = 1)), size = 2) +
  geom_node_label(aes(label = splitvar), size = 2, ids = "inner")

p_part <- ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_length_mm)) +
  geom_parttree(data = tree2, aes(fill = body_mass_g), alpha = 0.3) +
  geom_point(aes(col = body_mass_g)) +
  scale_colour_viridis_c(aesthetics = c("colour", "fill")) +
  theme(legend.position = "top")

p_tree + p_part
```

::: {style="font-size: 80%;"}
## Measures for model fit (node impurity)

Classification trees:

-   **Gini index** of leaf $m$: $$
    G_m =  \sum_{k=1}^K p_{mk}(1-p_{mk}),
    $$ $p_{mk}$: proportion of observations in region $R_m$ of class $k$.

-   **Entropy** of leaf $m$ $$
    D_m =  - \sum_{k=1}^K p_{mk} \log(p_{mk})
    $$
:::

::: {style="font-size: 80%;"}
## Measures for model fit (node impurity)

Regression trees, use **deviance** in leaf $m$ $$
\text{dev}_m =  \sum_{i \in m} (y_i-\mu_m)^2
$$ where

-   $i \in m$: Individuals in leaf $m$
-   $\mu_m$: Mean in leaf $m$
:::

## Overfitting

-   Regression trees tend to overfit
-   In principle, they could assign each observation to one leaf

## Tree pruning

A smaller tree with fewer splits may generalise better to new observations.

Solution: **Pruning**

![](assets/pruning.png){width="80%," fig-align="center"}

## Tree pruning

A smaller tree with fewer splits may generalise better to new observations.

Cost complexity pruning or weakest link pruning: Find tree $T$ $$
\underset{T}{argmin} \sum_{m=1}^{\mid T \mid }  \sum_{i \in m} (y_i-\mu_m)^2 + \alpha \mid T \mid
$$ where $\mid T \mid$ is the number of leaves and $\alpha$ a regularisation parameter.

## Tree pruning

Select the regularisation parameter $\alpha$ (cp in plot below) that produces the tree with the lowest node impurity (measured by deviance) as evaluated by cross-validation.

```{r complexity-plot}
plotcp(tree2)
```

## Decision trees in `R`: `rpart`

```{r decision-trees-r-1}
#| echo: false
#| eval: true

set.seed(123)
n <- 100

x1 <- runif(n)
x2 <- runif(n)
y <- ifelse(x1 + x2 > 1,
  x1 - x2, x1 + x2
) + rnorm(n,
  sd = 0.2
)
df <- data.frame(
  x1 = x1,
  x2 = x2, y = y
)
```

```{r decision-trees-r-2}
#| echo: true
#| eval: true
fit <- rpart(y ~ x1 + x2, data = df, control = list(
  cp = 0.01, # Any split that does not decrease the overall lack of fit by a factor of cp is not attempted
  minbucket = 5, # minimum number of observations in a terminal node
  maxdepth = 4, # maximum depth of any node
  xval = 10 # number of cross validation splits
))

summary(fit)
```

## Decision trees in `R`: `rpart`

```{r decision-trees-r-3}
#| echo: true
#| eval: true
#| fig-align: "left"
rpart.plot(fit)
```

## Decision trees in `R`: `rpart`

```{r decision-trees-r-4}
#| echo: true
#| eval: true

# cp is complexity parameter to which the rpart object will be trimmed.
# trained with cp 0.01
prune(fit, cp = 0.1)
```

## Decision trees in `R`: `tree`

```{r decision-trees-r-5}
#| echo: true
#| eval: true

x <- cbind(df$x1, df$x2) %>% as.matrix()
tree.out = tree(y ~ x)
plot(tree.out)
text(tree.out)
# tree.control(nobs, mincut = 5, minsize = 10, mindev = 0.01)
```

## Decision trees in `R`: `tree`

```{r decision-trees-r-6}
#| echo: true
#| eval: true

cv.tree(tree.out)
# prune.tree(tree.out, best)
# prune.tree(tree.out, k)
```

::: {style="font-size: 80%;"}
## Overview: decision trees

Advantages:

-   Interpretability
-   Intuitive, mirror human decision making
-   Allowing for non-linear effects

Disadvantages:

-   Overfitting is an issue
-   Highly unstable and variable, small changes in the input data can cause big changes in the tree structure
-   Minimal bias, but high variance

> **Ensemble methods**: Fit not one, but multiple trees.
:::
