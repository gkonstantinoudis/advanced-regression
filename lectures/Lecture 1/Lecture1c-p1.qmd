---
title: "Advanced Regression: Random effects and hierarchical models I"
date: "20-02-2023"
author: 
  name: "Garyfallos Konstantinoudis"
institute: "Imperial College London"
editor: visual
date: 2024-01-14
date-format: medium
title-slide-attributes:
  data-background-color: "#f3f4f4"
  data-background-image: "../../assets/bmeh_normal.png"
  data-background-size: 80%
  data-background-position: 60% 120%
format: 
    revealjs:
      theme: [default, ../../assets/style.scss]
      logo: "../../assets/bmeh_normal.png"
      slide-number: true
      incremental: false
      chalkboard:
        buttons: false
---

## Motivation

All methods presented so far assume that the observations are iid.

iid: Independent and identically distributed

-   **Independent**: The observations are independent from each other

    $$
     cor(x_i, x_{i'}) = 0  \text{ for all }  i,i' \in 1,….,n
    $$

-   **Identically**: All observations have the same distribution. For example when assuming a Normal distribution they all have the same mean and variance.

PS: Exchangeability: Allows for dependence between observations and only states that future observations behave like past ones.

## Motivation: How realistic is iid?

-   Often our data contains structure depending on how our data was sampled.

    -   Within $K$ boroughs in London we select $n$ participants ...

    -    From $K$ schools we sample $n$ students ...

    -   From $K$ hospitals we select $n$ patients ...

    -   At $K$ stores we sampled $n$ costumers ...

-    $k \in 1,…,K$ group index

**Grouping creates dependence**

Observations within a group are likely to be more similar to each other than to observations from other groups.

## Motivation: GP data

-   We are interested in the relationship of cholesterol and age and how age impacts cholesterol.

-   Sampling: We take measurements of patients from certain GPs.

    -    Group-level: GPs $K=12$

\\texttt{table(data.chol\[\["doctor"\]\])} \\\\

\\includegraphics\[scale=0.4\]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-table}

\\item Individual-level: Patients \$n=441\$

\\end{itemize}

\\end{itemize}

\\texttt{head(data.chol)} \\\\

\\centering \\includegraphics\[scale=0.4\]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-head}

}

## Generalised linear model (GLM)

-   Linear models can only model a quantitative outcome.

-   Quantitative outcomes are defined as a real number, taking possible values from $-\inf$ to $+\inf$.

-   Many important data types can by definition not be modelled using a linear model:

    -   Dichotomous or binary $\rightarrow$ only takes two values, 0 or 1

    -   Counts $\rightarrow$ only positive integers (0,1,2,3,...)

::: {.callout-note icon="false"}
Flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. Residuals are an important quantity for model diagnostics.
:::

## Binary outcome and logistic regression

Example: Case-control study

$$
y_i = 
\begin{cases}
1, \text{ If subject $i$ is a case} \\
0, \text{ If subject $i$ is a control} \\
\end{cases}
$$

```{r}
#| echo: false
#| eval: true
#| fig-width: 6
#| fig-height: 4
#| fig-align: "center"

library(lars)
library(dplyr)
library(ggplot2)

data(diabetes)
x = as.data.frame.matrix(diabetes$x)
y = ifelse(diabetes$y > mean(diabetes$y), 1, 0)

ggplot() + 
  geom_point(data = data.frame(x=x$glu, y = y), aes(x=x, y=y), size=2) + 
  theme_bw() + ylim(c(-0.5, 1.5)) + 
  ylab("light/severe diabetes") + xlab("glucose level") + 
  theme(text = element_text(size=20))

```

## Binary outcome and logistic regression

$$y = \underbrace{\alpha + \beta x}_{\text{Linear predictor}} + \epsilon$$

-   Linear predictor: $\eta=\alpha + \beta x$ is defined from $-\inf$ to $+\inf$.

-   But $y$ only $0$ or $1$ $\rightarrow$ The linear regression do not match the data well.

```{r}
#| echo: false
#| eval: true
#| fig-width: 6
#| fig-height: 4
#| fig-align: "center"

library(lars)
library(dplyr)
library(ggplot2)

data(diabetes)
x = as.data.frame.matrix(diabetes$x)
y = ifelse(diabetes$y > mean(diabetes$y), 1, 0)

coefs <- lm(y~glu, data = x) %>% coef()

ggplot() + 
  geom_point(data = data.frame(x=x$glu, y = y), aes(x=x, y=y), size=2) + 
  geom_abline(intercept = coefs[1], slope = coefs[2], lwd=1.3, col = "red") +
  theme_bw() + ylim(c(-0.5, 1.5)) + 
  ylab("light/severe diabetes") + xlab("glucose level") + 
  theme(text = element_text(size=20))
```

## How should we model this data?

**Key idea 1:** Instead of modelling the outcomes ($y=0$ or $y=1$) directly, logistic regression models the probability for $y=1$ denotes as

-   $P(y=1 \mid x)$

Notes on probabilities for binary data:

-   Probabilities can take values from 0 to 1

-   Probabilities are symmetric: $P(y=1\mid x) = 1- P(y=0\mid x)$

## How should we model this data?

**Key idea 2:** Transform the linear predictor $\eta= \alpha + \beta x_{i}$ (quantitative, can take values from $-\inf$ to $-inf$) to lie in the Interval $[0,1]$, which is valid for probabilities.

This can be achieved using the logit function: $\text{logit}(p) = \log (p/(1-p))$

```{r}
#| echo: false
#| eval: true
#| fig-width: 5
#| fig-height: 3
#| fig-align: "center"

library(ggplot2)

set.seed(11)
p = runif(n = 1000)

data.frame(
  p = p, 
  logitp = log(p/(1-p))
) %>% 
  ggplot() + geom_line(aes(x=p, y=logitp), lwd=1) + 
  theme_bw() + theme(text = element_text(size=15))

```

## Logistic regression

$$
\text{logit}(P(y=1 \mid x)) = \log(P(y=1 \mid x))/(1-P(y=1)\mid x) = \alpha + \beta x
$$

-   **Interpretation**: The regression coefficient $\beta$ in logistic regression represents the **log odds ratio** between $y=0$ and $y=1$.

-   **Estimation**: Maximum likelihood

## Technical details

-   Many important outcome types can be accommodated by GLMs.

-   Each of these distributions has a location parameter, e.g. $\mu$ for the Gaussian, $p$ for the Bernoulli and Binomial.

-   The natural link function between the location parameter and the linear predictor can be derived from the mathematical form of the distribution.

| Response    | Distribution | E(y)      | Link (g)       |
|-------------|--------------|-----------|----------------|
| Continuous  | Gaussian     | $\mu$     | $1$ (identity) |
| Dichotomous | Bernoulli    | $p$       | $\text{logit}$ |
| Counts      | Poison       | $\lambda$ | $\log$         |

<https://en.wikipedia.org/wiki/Generalized_linear_model>

## Technical details: GLM

The GLM consists of three elements:

1.  A probability distribution from the exponential family. Note: Only distributions that can be formulated as an exponential family can be modelled as GLM.
2.  A linear predictor $\eta=x\beta$
3.  A link function $g$ such that $E(y) = \mu = g^{-1}(\eta)$

## Technical details: Exponential families

An exponential family is a set of probability distributions of the following form:

$$
f_x(x \mid \theta) = h(x) \exp\{\eta(\theta) \times T(x) - A(\theta)\}
$$

where

-   $\theta$ is the parameter of interest.

-   $T(x)$ is a sufficient statistic.

-   $\eta(\theta)$ is the natural parameter or link function.

## Gaussian distribution as exponential distribution

Gaussian distribution with unknown $\mu$, but known $\sigma$:

$$
f_{\sigma}(x \mid \mu) =  \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{ -\frac{(x-\mu)^2}{2\sigma^2} \} =  \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{ -\frac{x^2}{2\sigma^2} + \frac{x\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2} \} 
$$

-   $\theta = \mu$

-   $h(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{ -\frac{x^2}{2\sigma^2} \}$

-   $T(x) = \frac{x}{\sigma}$

-   $\eta(\mu) = \frac{\mu}{\sigma}$

-   $A(\mu) = \frac{\mu^2}{2\sigma^2}$

## Logistic regression and binary outcomes

Binomial distribution with known number of trials $n$, but unknown probability $p$:

$$ \begin{align} f(x \mid p) &= {n\choose x} p^x (1-p)^{n-x} = \\ &= {n \choose x} \exp \{x \log (\frac{p}{1-p}) + n\log(1-p)\}  \end{align} $$

-   $\theta = p$

-   $h(x) = {n\choose x}$

-   $T(x)=x$

-   $\eta(p) = \log(\frac{p}{1-p})$

-   $A(p) = - n \log(1-p)$

## Logistic regression and binary outcomes

Formulate model: Three elements

1.  Error distribution for response variable
2.  Linear predictor
3.  Link function

The three elements of the logistic regression model are:

1.  The Bernoulli probability distribution modelling the data: $P( y_i= 1 \mid x_i ) = p_i$
2.  The linear predictor: $\alpha + \sum_{j=1}^p \beta_j x_{ij}$
3.  The link function $g$ associating the mean of $y$, $P( y_i= 1 \mid x_i )$ to the linear predictor: here the link is the [logistic link]{style="color:red;"} as we set $g(P( y_i= 1 \mid x_i ))=\text{logit}(p_i) = \beta_0 + \sum_{j=1}^p\beta_jx_{ij}$

## glm() in R

-   GLMs can be called in R just as linear models.

```{r}
#| echo: true
#| eval: true
library(lars);library(dplyr)

data(diabetes)
x = as.data.frame.matrix(diabetes$x)
y = ifelse(diabetes$y > mean(diabetes$y), 1, 0)

glm(y ~ age+sex+bmi+map+ltg, data = x, family=binomial) %>% summary()
```

## glm() in R

-   Different types of exponential families can be called using the $\text{family}$ option:

    -   $\text{binomial(link = 'logit')}$

    -   $\text{gaussian(link = 'identity')}$

    -   $\text{Gamma(link = 'inverse')}$

    -   $\text{poisson(link='log')}$

-   There are similar return values as for the $\text{lm}$ function:

    -   coefficients

    -   residuals

    -   fitted.values

    -   linear.predictors: the linear fit on the link scale

## Making predictions

1.  Train the prediction rule
2.  Derive predictions on the linear predictor scale for the new data

```{r}
#| echo: true
#| eval: true
#| warnings: false
#| fig-height: 4
library(lars);library(dplyr); library(ggplot2); library(patchwork)
set.seed(11)

data(diabetes)
x = as.data.frame.matrix(diabetes$x)
y = ifelse(diabetes$y > mean(diabetes$y), 1, 0)

glm_predict <- glm(y ~ glu, data = x, family=binomial) 
xnew <- data.frame(glu = rnorm(n=1000, mean = 0, sd = 0.5))
xnew %>% head()

eta = predict.glm(glm_predict, xnew)
```

## Plot the predictions

```{r}
#| echo: true
#| eval: true
#| warnings: false
ggplot() + geom_histogram(aes(x=eta))|
  ggplot() + geom_histogram(aes(x=exp(eta)/(exp(eta)+1)))
```

```{r}
#| echo: true
#| eval: true
rbinom(n = length(eta), 
       size = rep(1, length(eta)), 
       prob = exp(eta)/(exp(eta)+1)) %>% head()
```

## Take away: Generalised linear model

The model formulation in GLMs consists of three elements:

1.  Error distribution for response variable
2.  Linear predictor
3.  Link function

Most common data types can be modelled using GLMs

-   Continuous $\rightarrow$ Gaussian distribution

-   Dichotomous or binary $\rightarrow$ Bernoulli distribution

-   Counts $\rightarrow$ Poisson or Binomial (with known number of trial) distribution
