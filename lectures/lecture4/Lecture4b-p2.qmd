---
title: "Advanced Regression 4b: Machine learning, ensemble methods"
date: "03-14-2025"
author:
  name: "Garyfallos Konstantinoudis"
institute: "Imperial College London"
editor: visual
date-format: medium
title-slide-attributes:
  data-background-color: "#f3f4f4"
  data-background-image: "../../assets/logo_trans.png"
  data-background-size: 50%
  data-background-position: 90% 120%
format:
    revealjs:
      theme: [default, ../../assets/style.scss]
      logo: "../../assets/logo_trans.png"
      slide-number: true
      incremental: false
      chalkboard:
        buttons: false
---

```{r load-packages-and-data}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(patchwork)
library(mlbench)
library(rpart)
library(ggparty)
library(randomForest)
```


## Previous questions
-   How is FDR calculated in `sda.ranking`?
-   `caret::confusionMatrix` inconsistencies. 
-   Show how to do a ROC curve.
-   Maximum margin classifier.


**Ensemble methods:**

-   Bagging
-   Random forests
-   Boosting
-   Variable importance
-   Ensemble methods in `R`

::: {style="font-size: 70%;"}
## Bootstrap aggregation (bagging)

-   Let the original dataset contain $n$ samples
-   Take $n' < n$ repeated samples (with replacement) of the original dataset to create $B$ new datasets ($b \in 1,...,B$ bootstrapped training datasets).
-   Grow a tree on each bootstrapped training datasets.
-   Each of the trees has little bias, but high variance.

**Average over trees**

-   Regression tree: Compute the mean over the $B$ predictions $f_b$ of each individual regression tree $$
    f_{bag}(x) = \frac{1}{B} \sum_{b=1}^B f_b(x)
    $$
-   Classification tree: Majority vote (mode), the final prediction is the one that occurred most frequently among the $B$ trees
:::

## Bagging

```{r dag}
#| echo: false
#| eval: false
#| warning: false
#| message: false

# If figure does not render, store and load. 

library(DiagrammeR)

DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, fillcolor = Linen]

data [label = 'D', fillcolor = White]
data1 [label = 'D1', fillcolor = White]
data2 [label = 'D2', fillcolor = White]
data3 [label = '...', fillcolor = White, shape = none]
data4 [label = 'DB', fillcolor = White]
num1 [label = '1', fillcolor = White, shape = none, xlabel = M1]
num2 [label = '0', fillcolor = White, shape = none, xlabel = M2]
num3 [label = '1', fillcolor = White, shape = none, xlabel = MB]
res [label = '1', fillcolor = White, shape = none]

# edge definitions with the node IDs
data -> {data1 data2 data3 data4}
data1 -> num1
data2 -> num2
data4 -> num3
-> res
}")
```

![](assets/DAG.png){fig-align="center"}

::: {style="font-size: 70%;"}
## Example (taken from Prof Filippi lecture)

-   Aim: predict median house prices (Boston) based only on crime rate
-   Consider a tree with a single split at the root

```{r tree-boston-1}
#| echo: false
#| eval: true
#| fig-align: "center"
data(BostonHousing)

ctrl <- list(cp = 0, minbucket = 5, maxdepth = 1)
fit <- rpart(medv ~ log(crim), data = BostonHousing, control = ctrl)

p_data <- BostonHousing |>
  mutate(pred = predict(fit, BostonHousing)) |>
  ggplot(aes(x = log(crim), y = medv)) +
  geom_point(alpha = 0.2, size = 1) +
  geom_line(aes(y = pred), colour = "darkcyan", linewidth = .75) +
  theme_minimal() +
  labs(x = "crime score (log)", y = "median house price")

py2 <- as.party(fit)

p_tree <- ggparty(py2) +
  geom_edge(size = 0.6, colour = "grey60") +
  geom_edge_label(aes(label = prettyNum(breaks_label, digits = 1)), size = 5) +
  geom_node_label(aes(label = splitvar), size = 8, ids = "inner") #+
#   geom_node_label(aes(label = nodedata_medv),
#     ids = "terminal",
#     size = 8
#   )

p_data + p_tree
```
:::

::: {style="font-size: 70%;"}
## Example (taken from Prof Filippi lecture)

20 bootstrap samples

```{r tree-boston-2}
#| echo: false
#| eval: true
#| fig-align: "center"

BostonHousing <- BostonHousing %>%
  mutate(row = as.factor(rownames(.)))

fits <- NULL
# boostrap sampling with replacement
for (i in 1:20) {
  sample_d <- BostonHousing[sample(1:nrow(BostonHousing),
                                   nrow(BostonHousing), replace = TRUE), ]
  fit <- rpart(medv ~ log(crim), data = sample_d, control = ctrl)

  # Saving the coefficients
  fits[[i]] <- BostonHousing |>
    as_tibble() |>
    mutate(pred = predict(fit, BostonHousing)) |>
    mutate(sample = str_c(i))
}
fits <- bind_rows(fits)

p_data_boot <-
  ggplot() +
  geom_point(aes(x = log(crim), y = medv), alpha = 0.2, size = 1, data = BostonHousing) +
  geom_line(aes(x = log(crim), y = pred, colour = sample),
            data = fits, linewidth = .75) +
  labs(x = "crime score (log)", y = "median house price") +
  theme_minimal() +
  theme(legend.position = "none")

p_data + p_data_boot
```
:::

::: {style="font-size: 70%;"}
## Example (taken from Prof Filippi lecture)

Summarise the samples

```{r tree-boston-3}
#| echo: false
#| eval: true
#| fig-align: "center"

fits_summary <- fits |>
  group_by(row) |>
  summarise(pred = mean(pred)) |>
  left_join(BostonHousing)

p_data_boot_summary <-
  ggplot() +
  geom_point(aes(x = log(crim), y = medv), alpha = 0.2, size = 1,
             data = BostonHousing) +
  geom_line(aes(x = log(crim), y = pred), colour = "red",
            data = fits_summary, linewidth = .75) +
  labs(x = "crime score (log)", y = "median house price") +
  theme_minimal() +
  theme(legend.position = "none")

p_data_boot + p_data_boot_summary
```
:::

:::: {style="font-size: 90%;"}
## Out-of-bag error

-   Bagging has an intrinsic way of evaluating prediction performance

::: blockquote
**Out-of-bag samples**

-   The bootstrapped training dataset contains nâ€²samples drawn at random
-   $n-n'$ samples are left out. They are the out-of-bag samples.
:::

-   The out-of-bag error can be used to estimate the test error and prediction performance.
-   No cross-validation is needed.
::::

:::: {style="font-size: 80%;"}
## Random forest

-   Bagging always considers all $p$ variables to build a decision tree.
-   Consequently, individual trees in bagging may look very similar.
-   Random forest selects at random a subset of $m$ variables to be considered at each split.
-   Consequently, individual trees in random forests may look very different (or random).

::: blockquote
How to select $m$?

-   Regression random forest: $m=p/3$
-   Classification random forest: $m=\sqrt{p}$
-   Bagging: $m=p$
:::
::::

## 

::: {layout-ncol="2"}
![Bagging](assets/forest1.png){width="50%," fig-align="center"}

![Random forest](assets/forest2.png){width="50%," fig-align="center"}
:::

## Boosting of trees

-   Trees are grown sequentially, using information from previously grown trees.

-   Boosting learns from mistakes, after fitting the first tree, further trees are grown based on the residuals.

-   Tuning parameters:

    1.  $B$: Number of trees
    2.  $\lambda$: Learning rate
    3.  $d$: Number of splits per tree, interaction depth

::: {style="font-size: 80%;"}
## Boosting algorithm

1.  Initialise the output parameters: the predicted values $f(x)=0$ and the residuals $r_i=y_i$ for all observations $i$.
2.  Loop through $b \in 1,...,B$, repeat:
    1.  Fit tree $b$ with $d$ splits, obtain new predicted values $f_b(x)$
    2.  Update $$
        f(x) = f(x) + \lambda f_b(x)
        $$
    3.  $$
        r_i = r_i - \lambda f_b(x)
        $$
3.  Output as the final model the boosted predictions $$
    f_{boost}(x) = \sum_{b=1}^B \lambda f_b(x)
    $$
:::

## Outlook: gradient and XGboost

-   Gradient Boosting: A special case of boosting where errors are minimized by gradient descent algorithm.
-   eXtreme Gradient Boosting (XGBoost):
    -   Boosting algorithm, sequential learning
    -   Approximation and regularization
    -   Computationally efficient, based on parallel and distributed computing

## Variable importance: Understanding how algorithms work

**Interpretable machine learning**

-   Variable importance in ensemble trees allows to rank variables by their importance in the model.

::: {style="font-size: 70%;"}
## Variable importance: Understanding how algorithms work

There are two approaches for variable importance:

1.  **Permutation**: Mean decrease in accuracy
    -   First fit the model on the original data and evaluate the prediction error on the out-of-bag samples.
    -   Permute variable $j$, refit the model and re-evaluate the prediction error on the out-of-bag samples.
    -   The mean decrease of accuracy after permuting variable $j$ can be used to measure the importance of the $j$th variable.
2.  **Gini-index**: Mean decrease in impurity
    -   Record the decrease of impurity every time a tree is split at variable $j$.
    -   Average over all trees.
:::

## Bagging and random forests in `randomForest`

```{r rf}
#| echo: true
#| eval: false
randomForest(
  x, y, # train data
  xtest = NULL, ytest = NULL, # test data
  ntree = 500, # number of trees to grow
  # number of variables to consider in each tree
  mtry = if (!is.null(y) && !is.factor(y)) max(floor(ncol(x) / 3), 1) else floor(sqrt(ncol(x))),
  replace = TRUE, # sample with replacement
  importance = TRUE # computes variable importance
)
```

::: {style="font-size: 80%;"}
## Bagging and random forests in `randomForest`

`mtry` is number of variables to consider in each tree

-   Regression random forest: $m=p/3$
-   Classification random forest: $m=\sqrt{p}$
-   Bagging: $m=p$

```{r rf-fit}
#| echo: true
#| eval: true
data(BostonHousing)

# here, use formula rather than x and y
fit <- randomForest(medv ~ ., data = BostonHousing)
fit
```
:::

::: {style="font-size: 80%;"}
## Bagging and random forests in `randomForest`

-   `$predicted`: Predicted values
-   Classification measures: `$err.rate, $confusion, $votes`
-   Regression measures: `$mse, $rsq`
-   `$importance`: Variable importance (mean decrease in accuracy and mean decrease in impurity)

Further functions

-   `predict.randomForest`: Predict new data
-   `plot.randomForest`: Plot error against number of trees
-   `varImpPlot`: Plot variable importance
:::

## Bagging and random forests in `randomForest`

```{r rf-fit-plot}
#| echo: true
#| eval: true
#| fig-align: "center"
plot(fit)
```

## Bagging and random forests in `randomForest`

Mean decrease in impurity

```{r rf-vip-plot}
#| echo: true
#| eval: true
#| fig-align: "center"

varImpPlot(fit)
```

## Boosting in `gbm`

```{r gbm}
#| echo: true
#| eval: false
gbm(
  formula,
  distribution, # family of the outcome
  data,
  n.trees = 100, # number of trees to fit
  interaction.depth = 1, # maximum depth of each tree
  n.minobsinnode = 10, # minimum number of observations in the terminal nodes
  shrinkage = 0.1, # shrinkage factor
  cv.folds = 0 # performs additional cross-validation
)
```

Further functions

-   `predict.gbm`: Predict new data
-   `summary`: Relative influence

## Take away: Ensemble methods

-   Decision trees offer great interpretability.
-   But they tend to overfit, performing poorly in prediction.
-   Ensemble methods like bagging, random forest and boosting, fit many trees in different variations and average over them.
-   This reduces the variance and leads to greater generalizability.
-   Variable importance measures help to understand the contribution of specific variables

## Feedback

![](assets/Advanced%20Regression.png){width="100%," fig-align="center"}
