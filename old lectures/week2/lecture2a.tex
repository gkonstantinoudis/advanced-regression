\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%
\usepackage{listings}

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}


\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 2a Introduction to non-linear regression}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}




\date{28th February 2023}


\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}

%\section{Variable ranking, variable importance and variable selection}
%\section{Classical variable or model selection}
%\section{Variable importance}
%\section{Variable ranking}
%\section{Example: Genome-wide association studies}
%\section{Example: Differential expression}




\section{Introduction to non-linear regression}


\frame{
\frametitle{Non-linear regression} 

So far we have studied the linear regression models, which are very flexible tool for estimating relationships in a set of data. In particular, we have seen:

\begin{itemize}
\item Model set up, fitting, and residual diagnostics.
\item Model building and model comparison.
\item ANOVA/ likelihood ratio test.
\item Random intercepts and random slopes. 
\end{itemize}


\begin{block}{}
One of the most crucial assumptions of the previous lecture was that the associations between the outcome and the covariates are linear. What if the association was not linear?
\end{block}

}




\frame{
	\frametitle{A clarification} 
	
	In the name normal (or Poisson, binomial, etc.) linear model, the word 'linear' refers to the response being modelled as a linear combination of covariates, i.e.
	$$Y_i \sim N(\beta_1 + \beta_2X_{i2} + \dots + \beta_pX_{ip}, \sigma^2)$$
		
	It does not refer to each covariate-response relationship being linear. Therefore the following is also within the class of normal linear models
	
	
	$$Y_i \sim N(\beta_1 + \beta_2X_{i} + \beta_3X_{i}^2, \sigma^2)$$

	Here, the relationship between Y and x is quadratic, but it is still a linear model.
}
	
	


\frame{
\frametitle{So how do you tell if the relationship is linear?} 

\begin{itemize}
\item Plot each covariate against the response and see what shape the relationship is.
\item Make sure you plot it with the transformation used in the model. 
\item If linear, fit a linear model, if not start thinking of the shape of the relationship.
\item Fit as simple a model as possible, do not overcomplicate it unnecessarily. (Occam's razor)
\end{itemize}

}





\frame{
\frametitle{Example} 


\begin{columns}
	\column{.5\textwidth}
	\texttt{set.seed(11)\\
		x <- rnorm(n = 1000)\\
		y = 0.5*x*x + rnorm(1000, sd = 0.3)}
	\column{.5\textwidth}
	\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/quadrel.png}
\end{columns}

Associations are not solely linear.

}


\section{Basis Expansions}

\frame{
	\frametitle{Basis Expansions} 
	
	We need to define a set of flexible functions that could capture relationships that are not linear. In general, we can write:
	$$Y _i = \sum_j^K\beta_j \phi_j(X_j) + \epsilon_i $$
	
	which we can write: $f(X) = \beta^T\Phi(X)$ and we say that $\Phi(X)$ is a basis system for $f$.
}





\frame{
\frametitle{The polynomial basis function} 

$$\Phi(X) = (1) \text{ ,thus } Y _i = \beta_1 $$ 

\centering
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p1.png}
}


\frame{
	\frametitle{The polynomial basis function} 
	
	$$\Phi(X) = (1) \text{ ,thus } Y _i = \beta_0 + \epsilon_i $$ 
	
	\centering
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p1.png}
}

\frame{
	\frametitle{The polynomial basis function} 
	
	$$\Phi(X) = (1 \; X) \text{ ,thus } Y _i = \beta_0 + \beta_1 X + \epsilon_i$$ 
	
	\centering
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p2.png}
}

\frame{
	\frametitle{The polynomial basis function} 
	
	$$\Phi(X) = (1 \; X \; X^2) \text{ ,thus } Y _i = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon_i$$ 
	
	\centering
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p3.png}
}


\frame{
	\frametitle{The polynomial basis function} 
	
	$$\Phi(X) = (1 \; X \; X^2 \; X^3) \text{ ,thus } Y _i = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon_i$$ 
	
	\centering
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p4.png}
}


\frame{
	\frametitle{The polynomial basis function} 
	
	$$\Phi(X) = (1 \; X \; X^2 \; X^3 \; X^4) \text{ ,thus } $$ 
	$$Y _i = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 X^4 + \epsilon_i$$ 
	
	\centering
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p5.png}
}



\frame{
	\frametitle{The polynomial basis function} 
	
	$$\Phi(X) = (1 \; X \; X^2 \; X^3 \; X^4 \; X^5) \text{ ,thus } $$ 
	$$Y _i = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 X^4 +\beta_5 X^5 + \epsilon_i $$ 
	
		\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p6.png}
}


\frame{
	\frametitle{Example} 
	
	
	\begin{columns}
		\column{.5\textwidth}
		\texttt{set.seed(11)\\
			x <- rnorm(n = 1000)\\
			y = 0.5*x*x + rnorm(1000, sd = 0.3)}
		\column{.5\textwidth}
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/quadrel.png}
	\end{columns}
	
}

% Radial basis function, rectified linear, hyperbolic tangent, spatial basis functions

\begin{frame}[fragile]{Fit a polynomial in R} 
	
	Use the function \texttt{I()} in the \texttt{lm} call: \\
	\begin{lstlisting}[language=R, basicstyle=\tiny]
lm(y ~ x + I(x^2) + I(x^3)) %>% summary()

Call:
lm(formula = y ~ x + I(x^2) + I(x^3))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.14539 -0.20239  0.01008  0.20206  0.95031 

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.012156   0.011541  -1.053    0.292    
x            0.007367   0.014781   0.498    0.618    
I(x^2)       0.511645   0.006724  76.096   <2e-16 ***
I(x^3)      -0.004603   0.003786  -1.216    0.224    
---

Residual standard error: 0.2982 on 996 degrees of freedom
Multiple R-squared:  0.8547,	Adjusted R-squared:  0.8542 
F-statistic:  1952 on 3 and 996 DF,  p-value: < 2.2e-16
	\end{lstlisting}

\end{frame}



\begin{frame}[fragile]{How can we test the Linearity assumption?} 
	\begin{itemize}
		\item Notice that the Linear model is nested in the Quadratic curve, so can do Likelihood Ratio Test (F test for Gaussian data)
		\item The same holds for Cubic Vs Quadratic Vs Linear
	\end{itemize}
	\begin{lstlisting}[language=R, basicstyle=\tiny]
mod1 <- lm(y ~ x + I(x^2))
mod2 <- lm(y ~ x + I(x^2) + I(x^3))
anova(mod1, mod2)

Analysis of Variance Table

Model 1: y ~ x + I(x^2)
Model 2: y ~ x + I(x^2) + I(x^3)
Res.Df    RSS Df Sum of Sq      F Pr(>F)
1    997 88.702                           
2    996 88.571  1   0.13147 1.4784 0.2243
	\end{lstlisting}
	

\end{frame}

\frame{
	\frametitle{Pros and cons} 
	Pros
	\begin{itemize}
		\item These curves are quite flexible—a quadratic can fit most biologically plausible curves
		\item The curves only use 1(quadratic) or 2(cubic) degrees of freedom more than linear, unlike dummy variable models
		\item The results are not sensitive to choice of boundaries (there aren’t any)
		\item Outliers mostly influence the extremes of the curve, not the center part		
	\end{itemize}
	Cons
\begin{itemize}
	\item They use more degrees of freedom than linear, and therefore have less power
	\item There is still some sensitivity to influential observations		
\end{itemize}
}

\frame{
\frametitle{Other types of basis function} 
	\begin{itemize}
	\item \textbf{Harmonics} - The Fourier basis function
	$$1, sin(\omega X), cos(\omega X), sin(2\omega X), cos(2\omega X), \dots, $$
	$$sin(m \omega X), cos(m \omega X)$$
	\item constant $\omega$ defines the period of oscillation of the first
	sine/cosine pair. This is $\omega = 2\pi/P$ where $P$ is the period.
\end{itemize}

}



\frame{
	\frametitle{Example: Fourier basis function} 
		$$\Phi(X) = (1 \; sin(\omega X) \; cos(\omega X) \; sin(2\omega X) \; cos(2\omega X)) \text{ ,thus } $$ 
	$$Y _i = \beta_0 + \beta_1 sin(\omega X) + \beta_2 cos(\omega X) + \beta_3 sin(2 \omega X) + \beta_4 cos(2\omega X) + \epsilon_i $$ 
	
	\includegraphics[width = 11cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p7.png}
	
}

\frame{
	\frametitle{Pros and cons} 
	Pros
	\begin{itemize}
		\item Excellent computational properties, especially if the observations are equally spaced.
		\item Natural for describing periodic data, such as the annual weather cycle
		\item The results are not sensitive to choice of boundaries (there aren’t any)	
	\end{itemize}
	Cons
	\begin{itemize}
		\item functions are periodic; this can be a problem if the data are,
		for example, growth curves.	
	\end{itemize}
}



\frame{
	\frametitle{Parametric non-linear effects} 
	\begin{itemize}
		\item The following models captured the non-linear relationships
		using simple non-linear functions.
		\item These types of models are to be preferred if possible, because
		again they are simpler to make inference from, e.g. the
		relationship is quadratic.
		\item However, there may be times when the relationship being
		modelled does not look like a parametric function. Then what
		should you do?
	\end{itemize}

}


\frame{
	\frametitle{Parametric non-linear effects} 
	Consider the following form:
	
	\includegraphics[width = 7cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p8.png}
	
	It doesn't look like any simple parametric form (e.g. polynomial,
	sinusoidal, etc), so what do you do?
}

\frame{
\frametitle{Smooth functions} 

\begin{itemize}
\item There are many methods to estimate non-linear relationships
such as that on the previous slide.
\item They are generically called smooth functions, and include
splines (lots of different types), kernel smoothers and local
linear smoothers.
\item We will focus on splines in this lab, because they are simple to
understand graphically and are easy to fit to data.
\end{itemize}

}

\section{Splines}
\frame{
	\frametitle{Splines} 
	
	\begin{itemize}
		\item Splines were originally thin splints of wood used to trace complex, smooth curves in engineering and architectural.
		\item Splines were pinned to the drawing at points where the spline changed its curvature.
	\end{itemize}
\centering
		\includegraphics[width = 7cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/splinesex.png}
}


\frame{
	\frametitle{Piecewise Linear Splines} 
	
	\begin{itemize}
		\item We begin fitting splines by dividing the range of exposure into pieces.
		\item Instead of fitting a constant in each piece, we fit a separate linear term in each piece
		\item This has more power since it allows variation within categories to predict variation in outcome
		\item We can use fewer categories to capture the deviation from a strictly linear curve because we have slopes within category.
	\end{itemize}
}



\begin{frame}[fragile]{Linear threshold in R}
	\vspace{-0.2cm}
\begin{lstlisting}[language=R, basicstyle=\tiny]
set.seed(11)
x <- rnorm(n = 1000)
y <- numeric(1000)
y[x<0] <- 1 + 0.5*x[x<0] + 0.2*x[x<0]^2 + rnorm(n = length(x[x<0]), sd = 0.2)
y[x>=0] <- 0.5 + sin(2*pi/2 * x[x>=0])  + rnorm(n = length(x[x>0]), sd = 0.3)

dat <- data.frame(x=x, y=y)
lm(y~x:(x<=-1) + x:(x<=0.5) + x:(x<=1.5) + x:(x<=3), data=dat) -> mod1

dat %>% mutate(
	mean = mod1$fitted.values, 
	disc = case_when(dat$x<=-1~1, dat$x>-1 & dat$x<=0.5~2, 
	dat$x>0.5 & dat$x<=1.5~3, dat$x>1.5 & dat$x<=3~4, dat$x>3~5)) %>% 
ggplot() + geom_point(aes(x = x, y = y), cex = .7, alpha = 0.2) + 
	geom_line(aes(x = x, y = mean, group = disc), cex = 0.8) + theme_bw() 
\end{lstlisting}
	\begin{figure}
	\centering	
\includegraphics[width = 5cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p9.png}	
\end{figure}

\end{frame}

\begin{frame}[fragile]{Linear splines in R}
	\begin{lstlisting}[language=R, basicstyle=\tiny]
library(splines)
fit <- lm( y ~ bs(x, degree=1, knots=c(-1, 0.5, 1.5, 3)), data = dat)

ggplot() + 
geom_point(aes(x = x, y = y), cex = .6, col = cols[1]) + theme_bw() + 
xlab("") + ylab("") +
geom_line(aes(x=dat$x, y = fit$fitted.values))
	\end{lstlisting}
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p10.png}	
	\end{figure}
	Ensure continuity in the knots, but how can we select knots?
\end{frame}

\begin{frame}{Linear splines in R}
	Linear threshold model:
	\begin{align*}
		Y_i &= \beta_0^{(1)} + \beta_1^{(1)}x + \epsilon \; \; \; \; \; \; x \leq -1\\
		Y_i &= \beta_0^{(2)} + \beta_1^{(2)}x + \epsilon \; \; \; \; \; \; -1< x \leq 0.5\\
		Y_i &= \beta_0^{(3)} + \beta_1^{(3)}x + \epsilon \; \; \; \; \; \; 0.5<x \leq 1.5 \\
		Y_i &= \beta_0^{(4)} + \beta_1^{(4)}x + \epsilon \; \; \; \; \; \; 1.5<x \leq 3 \\
		Y_i &= \beta_0^{(5)} + \beta_1^{(5)}x + \epsilon \; \; \; \; \; \; x\geq 3 
	\end{align*}
	\vspace{-.2cm}
	Linear spline model:
	\begin{align*}
	Y_i = \beta_0 + \beta_1 x +  \beta_2(x+1)_+ +& \beta_3(x-0.5)_+ + \beta_4 (x-1.5)_+ \\ &+ \beta_5 (x-3)_+ + \epsilon \\
	(x-k)_+& = 
	\begin{cases}
	0, x<k \\
	x-k, x\geq k \end{cases}
\end{align*}
\end{frame}

\begin{frame}[fragile]{Linear splines in R without a package}
	\begin{lstlisting}[language=R, basicstyle=\tiny]
lm(y~ x + I((x+1)*(x>=-1) )+ I((x-0.5)*(x>=0.5)) + 
    I((x-1.5)*(x>=1.5)) + I((x-3)*(x>=3)), data=dat) -> mod2

ggplot() + 
geom_point(aes(x = x, y = y), cex = .6, col = cols[1]) + theme_bw() + 
xlab("") + ylab("") +
geom_line(aes(x=dat$x, y = mod2$fitted.values))

	\end{lstlisting}
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p11.png}	
	\end{figure}
\end{frame}

\section{Cubic splines}

\begin{frame}[fragile]{Cubic splines}
	Similarly we can define higher order polynomial splines, for instance:
	\begin{align*}
	Y_i = \beta_0 &+ \beta_1 x +  \beta_2x^2 + \beta_3x^3 + \\
	 &\beta_4 (x+1)_+ +\beta_5 (x+1)_+^2 +\beta_6 (x+1)_+^3 +\\ 
	 &\beta_7 (x-0.5)_+ +\beta_8 (x-0.5)_+^2 +\beta_9 (x-0.5)_+^3 + \\ & \dots + \epsilon \\
	(x-k)_+& = 
	\begin{cases}
	0, x<k \\
	x-k, x\geq k \end{cases}
	\end{align*}
	which reduces to the following to ensure smooth curvature on the knots (it can be seen after deriving the first and second derivative):
	\begin{align*}
	Y_i = \beta_0 &+ \beta_1 x +  \beta_2x^2_+ + \beta_3x^3 + \\
	&+\beta_6 (x+1)_+^3 + \beta_9 (x-0.5)_+^3 +  \dots + \epsilon \\
	\end{align*}
	
\end{frame}


\begin{frame}[fragile]{Cubic splines in R}
	\begin{lstlisting}[language=R, basicstyle=\tiny]
fit_c <- lm( y ~ bs(x, degree=3, knots=c(-1, 0.5, 1.5, 3)), data = dat)
	\end{lstlisting}
	
	similarly without the \texttt{splines} package:
	
	\begin{lstlisting}[language=R, basicstyle=\tiny]
fit_nopack <- lm(
y ~ x + I(x^2) + I(x^3) + I((x+1)^3*(x>=-1)) + I((x-0.5)^3*(x>=0.5)) +
      I((x-1.5)^3*(x>=1.5)) + I((x-3)^3*(x>=3)), data = dat)
	\end{lstlisting}
	
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p12.png}	
	\end{figure}
\end{frame}


\begin{frame}[fragile]{Splines}
	 A spline of order n is a piecewise polynomial function of degree 
	n-1 in a variable x.\\
	(Basis) Splines can be:
\begin{itemize}
	\item Piecewise constant
	\item Linear
	\item Quadratic
	\item Cubic
	\item higher order polynomials
\end{itemize}
Why are the borders so wiggly?
\end{frame}

\section{Natural splines}
\begin{frame}[fragile]{Natural cubic splines}
	\begin{itemize}
		\item Splines can have high variance in the boundaries
		\item This problem aggravates if knots too few
		\item The natural spline constraints the fit to be linear before and after the first and last knots
		\item This stabilises the fitting and makes a more reasonable assumption.
	\end{itemize}
\end{frame}


\begin{frame}[fragile]{Natural cubic splines in R}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
fit_ns <- lm( y ~ ns(x, knots=c(-1, 0.5, 1.5, 3)), data = dat)
	\end{lstlisting}
	\begin{figure}
	\centering	
	\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p13.png}	
	\end{figure}
\end{frame}


\begin{frame}[fragile]{Well, better fit can be achieved with more knots}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
	fit_ns <- lm( y ~ ns(x, df=10), data = dat)
	\end{lstlisting}
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p14.png}	
	\end{figure}
\end{frame}


\begin{frame}[fragile]{... and more}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
	fit_ns <- lm( y ~ ns(x, df=50), data = dat)
	\end{lstlisting}
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p15.png}	
	\end{figure}
\end{frame}

\begin{frame}[fragile]{... and more}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
	fit_ns <- lm( y ~ ns(x, df=100), data = dat)
	\end{lstlisting}
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p16.png}	
	\end{figure}
\end{frame}

\begin{frame}[fragile]{... and more}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
	fit_ns <- lm( y ~ ns(x, df=1000), data = dat)
	\end{lstlisting}
	\begin{figure}
		\centering	
		\includegraphics[width = 6cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/p17.png}	
	\end{figure}

is this a useful model?
\end{frame}

\begin{frame}{Summary}
	\begin{itemize}
		\item Introduction to different basis function and how to fit them in R
		\item Theory and application of linear splines
		\item Understand more flexible non-parametric functions (focus on splines)
	\end{itemize}
Questions?
\end{frame}

\end{document}





