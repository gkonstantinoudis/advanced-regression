\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}


\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 4b Machine learning: Ensemble methods}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}



\date{14th March 2023}

\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}


\section{Ensemble methods}

\subsection{Bagging}

\frame{
\frametitle{Bootstrap aggregation (bagging)}

\begin{itemize}
\item Let the original dataset contain $n$ samples	
\item Take $n' < n$ repeated samples (with replacement) of the original dataset to create $B$ new datasets ($b \in 1,...,B$ bootstrapped training datasets).
\item Grow a tree on each bootstrapped training datasets.
\item Each of the trees has little bias, but high variance. 
\end{itemize}

\begin{block}{Average over trees}
\begin{itemize}
\item Regression tree: Compute the mean over the $B$ predictions $f_b$ of each individual regression tree
\vspace{-0.2cm}
 \begin{equation}
f_{bag}(x) = \frac{1}{B} \sum_{b=1}^B f_b(x) \nonumber
\end{equation}
\vspace{-0.4cm}
\item Classification tree: Majority vote (mode), the final prediction is the one that occurred most frequently among the $B$ trees
\end{itemize}
\end{block}

}


\frame{
	\frametitle{Bagging}
	
\includegraphics [width=0.7\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/bagging (2)} 
	
	
}



\frame{
	\frametitle{Example (taken by Prof Filippi lecture)}
	
	\begin{itemize}
		\item Regression for Boston housing data
		\item Aim: predict median house prices based only on crime rate
		\item Consider a tree with a single split at the root
	\end{itemize}
\centering
\includegraphics [width=0.8\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/bagex1.PNG} 

}

\frame{
	\frametitle{Example (taken by Prof Filippi lecture)}
	
	\begin{itemize}
		\item 20 Bootstrap samples
	\end{itemize}
	\centering
	\includegraphics [width=0.8\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/bagex2.PNG} 
	
}

\frame{
	\frametitle{Example (taken by Prof Filippi lecture)}
	
	\begin{itemize}
		\item Summarize the samples
	\end{itemize}
	
	\includegraphics [width=0.8\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/bagex3.PNG} 
	
}



\frame{
\frametitle{Out-of-bag error}

\begin{itemize}
\item Bagging has an intrinsic way of evaluating prediction performance
\begin{block}{Out-of-bag samples}
\begin{itemize}
\item[$\diamond$] The bootstrapped training dataset contains $n'$ samples drawn at random.
\item[$\diamond$] $n-n'$ samples are left out. They are the out-of-bag samples.
\end{itemize}
\end{block}
\item The out-of-bag error can be used to estimate the test error and prediction performance.
\item No cross-validation is needed. 
\end{itemize}


}


\subsection{Random forests}


\frame{
\frametitle{Random forest}

\begin{itemize}
\item Bagging always considers all $p$ variables to build a decision tree.
\item Consequently, individual trees in bagging may look very similar. 
\item Random forest selects at random a subset of $m$ variables to be considered at each split. 
\item Consequently, individual trees in random forests may look very different (or random).
\end{itemize}



\begin{block}{How to select $m$?}
\begin{itemize}
\item Regression random forest: $m=p/3$
\item Classification random forest: $m=\sqrt(p)$
\item Bagging: $m=p$
\end{itemize}
\end{block}

}

\frame{
%\frametitle{Random forest}
 
\begin{columns}
\begin{column}{0.7\textwidth}
\begin{center} 
\includegraphics [width=0.7\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/bamboo} 
\end{center}
\end{column}
\begin{column}{0.28\textwidth}
Bagging 
\end{column}
\end{columns}


\begin{columns}
\begin{column}{0.7\textwidth}
\begin{center} 
\includegraphics [width=0.7\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/amazon.jpg} 
\end{center}
\end{column}
\begin{column}{0.28\textwidth}
Random forest
\end{column}
\end{columns}


}


\subsection{Boosting}

\frame{
\frametitle{Boosting of trees}

\begin{itemize}
\item Trees are grown sequentially, using information from previously grown trees.
\item Boosting learns from mistakes, after fitting the first tree, further trees are grown based on the residuals.
\item Tuning parameters
\begin{enumerate}
\item $B$: Number of trees
\item $\lambda$: Learning rate
\item $d$: Number of splits per tree, interaction depth
\end{enumerate}
\end{itemize}
\centering
\includegraphics [width=0.8\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/boost1.png} 

}



\frame{
\frametitle{Boosting of trees}

Boosting algorithm:
\begin{enumerate}
\item Initialise the output parameters: the predicted values $f(x)=0$ and the residuals $r_i=y_i$ for all observations $i$.
\item Loop through $b \in 1,...,B$, repeat:
\begin{enumerate}
\item[(a)] Fit tree $b$ with $d$ splits, obtain new predicted values $f_b(x)$ 
\item[(b)] Update 
 \begin{equation}
f(x) = f(x) + \lambda f_b(x) \nonumber
\end{equation}
\item[(c)]
 \begin{equation}
r_i = r_i - \lambda f_b(x) \nonumber
\end{equation}
\end{enumerate}
\item Output as the final model the boosted predictions
 \begin{equation}
f_{boost}(x) = \sum_{b=1}^B \lambda f_b(x) \nonumber
\end{equation}
\end{enumerate}


}




\frame{
\frametitle{Outlook: Gradient and XGboost}

\begin{itemize}
\item Gradient Boosting: A special case of boosting where errors are minimized by gradient descent algorithm.
\item eXtreme Gradient Boosting (XGBoost): 
\begin{itemize}
\item Boosting algorithm, sequential learning
\item Approximation and regularization
\item Computationally efficient, based on parallel and distributed computing
\end{itemize}
\end{itemize}

}





\subsection{Variable importance}

\frame{
\frametitle{Variable importance: Understanding how algorithms work}


\begin{block}{Interpretable machine learning}
\begin{itemize}
\item Variable importance in ensemble trees allows to rank variables by their importance in the model.
\end{itemize}
\end{block}

There are two approaches for variable importance:
\begin{enumerate}
\item \textbf{Permutation: Mean decrease in accuracy}
\begin{itemize}
\item First fit the model on the original data and evaluate the prediction error on the out-of-bag samples.
\item Permute variable $j$, refit the model and re-evaluate the prediction error on the out-of-bag samples.
\item The mean decrease of accuracy after permuting variable $j$ can be used to measure the importance of the $j$th variable. 
\end{itemize}
\item \textbf{Gini-index: Mean decrease in impurity} 
\begin{itemize}
\item Record the decrease of impurity every time a tree is split at variable $j$.
\item Average over all trees.
\end{itemize}
\end{enumerate}
}




\subsection{Ensemble methods in \texttt{R}}


\frame{
\frametitle{Bagging and random forests in \texttt{library(randomForest)}}

\texttt{randomForest(\textcolor{red}{x, y, xtest=NULL, ytest=NULL}, \\
				\textcolor{green}{ntree=500}, \\
				\textcolor{blue}{mtry=if (!is.null(y) $\&\&$ !is.factor(y)) max(floor(ncol(x)/3), 1) \\
				else floor(sqrt(ncol(x)))}, \\
                \textcolor{gray}{replace=TRUE, importance=TRUE)}}

\begin{itemize}
\item \textcolor{red}{\texttt{x,y}}: training data
\item \textcolor{red}{\texttt{xtest,ytest}}: test data (optional, usually not available)
\item \textcolor{green}{\texttt{ntree}}: number of trees to grow
\item \textcolor{blue}{\texttt{mtry}}: number of variables to consider in each tree
%\begin{itemize}
%\item Regression random forest: $m=p/3$
%\item Classification random forest: $m=\sqrt(p)$
%\item Bagging: $m=p$
%\end{itemize}
\item  \textcolor{gray}{\texttt{replace}}: sample with replacement
\item  \textcolor{gray}{\texttt{importance=TRUE}}: computes variable importance also on out-of-bag samples
\end{itemize}

}


\frame{
\frametitle{Bagging and random forests in \texttt{library(randomForest)}}


\texttt{mtry} number of variables to consider in each tree
\begin{itemize}
\item Regression random forest: $m=p/3$
\item Classification random forest: $m=\sqrt(p)$
\item Bagging: $m=p$
\end{itemize}
\vspace{-0.2cm}
\begin{center} 
\includegraphics [width=0.55\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/randomForest1} 
\end{center}

}


\frame{
\frametitle{Bagging and random forests in \texttt{library(randomForest)}}
Values:
\begin{itemize}
\item \texttt{$\$$predicted}: Predicted values
\item Classification measures: \texttt{$\$$err.rate, $\$$confusion, $\$$votes}
\item Regression measures: \texttt{$\$$mse, $\$$rsq}
\item \texttt{$\$$importance}: Variable importance (mean decrease in accuracy and mean decrease in impurity)
\end{itemize}

Further functions
\begin{itemize}
\item \texttt{predict.randomForest}: Predict new data
\item \texttt{plot.randomForest}: Plot error against number of trees
\item \texttt{varImpPlot}: Plot variable importance
\end{itemize}

}



\frame{
\frametitle{Bagging and random forests in \texttt{library(randomForest)}}

\texttt{plot.randomForest}
\begin{center} 
\includegraphics [width=0.95\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/plot_rf.jpg} 
\end{center}

}




\frame{
\frametitle{Bagging and random forests in \texttt{library(randomForest)}}

\texttt{varImpPlot(rf.out, type)}
\begin{itemize}
\item \texttt{type=1}: Mean decrease in accuracy
\item \texttt{type=2}: Mean decrease in impurity
\end{itemize}
\begin{center} 
\includegraphics [width=0.95\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/plot_varimp.jpg} 
\end{center}

}


\frame{
\frametitle{Boosting in \texttt{gbm}}

\texttt{gbm(\textcolor{red}{formula, distribution, data},  \\ 
	\textcolor{green}{n.trees = 100, interaction.depth = 1, n.minobsinnode = 10,} \\
	\textcolor{blue}{shrinkage = 0.1, cv.folds = 0)}}

\begin{itemize}
\item \textcolor{red}{\texttt{formula}}:  $y \sim .$
\item \textcolor{red}{\texttt{distribution}}:  family of the outcome
\item \textcolor{red}{\texttt{data}}:  data.frame
\item \textcolor{green}{\texttt{n.trees}}: number of trees to fit
\item \textcolor{green}{\texttt{interaction.depth}}: maximum depth of each tree
\item \textcolor{green}{\texttt{n.minobsinnode}}: minimum number of observations in the terminal nodes
\item \textcolor{blue}{\texttt{shrinkage}}: shrinkage factor
\item \textcolor{blue}{\texttt{cv.folds > 0}}: performs additionally cross-validation 
\end{itemize}

}



\frame{
\frametitle{Boosting in \texttt{mboost}}


Further functions
\begin{itemize}
\item \texttt{predict.gbm}: Predict new data
\item \texttt{summary}: Relative influence
\end{itemize}
\vspace{-0.2cm}
\begin{center} 
\includegraphics [width=0.64\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/plot_gbm.jpg} 
\end{center}

}





\frame{
\frametitle{Take away: Ensemble methods} 


\begin{itemize}
\item Decision trees offer great interpretability.
\item But they tend to overfit, performing poorly in prediction.
\item Ensemble  methods like bagging, random forest and boosting, fit many trees in different variations and average over them.
\item This reduces the variance and leads to greater generalizability.
\item Variable importance measures help to understand the contribution of specific variables. 
\end{itemize}

\begin{block}{Thank you Deborah Schneider-Luftman for material and graphs}
\end{block}

}



\frame{
\frametitle{General background reading} 


\includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ISL2}

\vspace{0.2cm}
\begin{itemize}
\item An Introduction to Statistical Learning: 
8 Tree-Based Methods
\item \url{http://faculty.marshall.usc.edu/gareth-james/ISL/}
\end{itemize}

}














\end{document}










