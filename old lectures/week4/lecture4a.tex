\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\usepackage{multirow, colortbl}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

\newcommand{\nc}{\newcommand}
\nc{\pr}{\ensuremath{\mathbb{P}}}
\urldef{\technical}\url{http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29}

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}
\usepackage{listings}

\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 4a Machine learning: Classification}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}



\date{14th March 2023}

\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}

\section{Classification analysis}
\subsection{Basic concepts}

\frame{
\frametitle{Classification of binary outcomes} 
\begin{itemize}
\item Assume we have a binary outcome $y_i$ for subject $i$.
\end{itemize}
\begin{equation}
y_i = \begin{cases}
1 \text{ if subject } i  \text{ is a case} \\
0 \text{ if subject } i  \text{ is a control} \\
\end{cases}  \nonumber
\end{equation}
\begin{itemize}
\item Additionally, we have for each subject a predictor matrix $x_i$ including $p$ predictors or features.
\end{itemize}

\begin{block}{$K=2$ Classification of two classes}
Aim is to predict the outcome of a new observation $i$ based on $x_i$ and to assign observation $i$ to either class $k=0$ or $k=1$. Decisions are based on the probability to belong to class $k$ conditional on $x_i$.
\begin{equation}
\pr(y_i=k \mid x_i) \nonumber
\end{equation}
\end{block}

}




\subsection{Evaluation}

\frame{
\frametitle{Evaluation of classification performance: Confusion matrix} 


\begin{table}
\begin{tabular}{c c | c c |}   
 & & \multicolumn{2}{ c |}{Unobserved truth} \\
 &  & Negative & Positive   \\
\hline
\multirow{4}{*}{Declared} & Negatives   &  \cellcolor{green} True Negative  & \cellcolor{red} False negative   \\ 
 &   &  \cellcolor{green} (TN) & \cellcolor{red}  (FN)   \\ 
 & Positives    &  \cellcolor{red} False positive   &   \cellcolor{green}  True Positive  \\ 
 &     &  \cellcolor{red}  (FP)   &   \cellcolor{green}   (TP) \\ 
 \hline
 Total &     &  N   &   P \\ 
 \hline
\end{tabular}
\end{table}

\begin{block}{Confusion matrix:}
Collection of TP, FP, TN, FN of a classifier.
\end{block}

}

\frame{
\frametitle{Characteristics derived from the confusion matrix} 
\begin{itemize}
\item \textbf{Sensitivity}, recall, or true positive rate (TPR): 
\begin{equation}
TPR = \frac{TP}{P} = \frac{TP}{TP+FN} \nonumber
\end{equation}
\item \textbf{Specificity}, or true negative rate (TNR):
\begin{equation}
TNR = \frac{TN}{N} = \frac{TN}{TN+FP} = 1-FPR \nonumber
\end{equation}
\item \textbf{False positive rate}  (FPR):
\begin{equation}
FPR = \frac{FP}{N} = \frac{FP}{TN+FP} = 1-TNR \nonumber
\end{equation}
\item \textbf{Precision}, or positive predictive value (PPV):
\begin{equation}
PPV = \frac{TP}{\text{Declared } P} = \frac{TP}{TP+FP} = 1-FDR \nonumber
\end{equation}
\end{itemize}
}

\frame{
	\frametitle{Interpretation} 
	\begin{itemize}
		\item \textbf{Sensitivity}, or true positive rate (TPR): The probability that the test result picks up the disease.
		\item \textbf{Specificity}, or true negative rate (TNR):The probability that the test result identifies the ones that do not have the disease.
		\item \textbf{False positive rate}  (FPR): the expectancy of the false positive ratio, i.e. how likely is to have a negative test given that the patient has the disease.
		\item \textbf{Precision}, or positive predictive value (PPV): how likely it is for someone to truly have the disease, in case of a positive test result.
	\end{itemize}
}

\frame{
\frametitle{Receiver operating characteristic (ROC)} 
\begin{itemize}
\item Plot sensitivity (TPR) on the $y$-axis against 1-specificity (FPR) on the $x$-axis.
\item Area under the ROC curve as summary-statistic for prediction performance.
\end{itemize}

\centering \includegraphics [scale=0.33]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ROC-example.jpeg}


}


\frame{
%\frametitle{Receiver operating characteristic (ROC)} 

\centering \includegraphics [scale=0.3]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/AUC-ROC}
\begin{itemize}
\item Tweetorial on AUC \url{https://twitter.com/cecilejanssens/status/1104134423673479169}
\end{itemize}
    
}



\frame{
\frametitle{Precision-recall curve} 
\begin{itemize}
\item Plot precision or positive predictive value (PPV) on the $y$-axis against the recall or true positive rate (TPR) on the $x$-axis.
\end{itemize}

\centering \includegraphics [scale=0.33]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/PR-example.jpeg}

}


\frame{
\frametitle{ROC and precision-recall curve in R} 
\begin{enumerate}
\item Package \texttt{pROC}: \\
\begin{itemize}
\item ROC plot using the function \texttt{roc(true.y,predicted.y)}
\end{itemize}
\item Package \texttt{PRROC}: \\
\begin{itemize}
\item ROC plot using the function \texttt{roc.curve(fg,bg,curve=TRUE)}
\item Precision-recall plot using the function \texttt{pr.curve(fg,bg,curve=TRUE)}
\item Note: \text{fg} is a prediction score for group 1 and \text{bg} is a prediction score for group 0. 
\end{itemize}
\end{enumerate}
}

\section{Discriminant analysis}


\frame{
	\frametitle{Motivation of discriminant analysis} 
	
	\begin{figure}
		\centering \includegraphics [scale=0.33]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/damotive}
	\end{figure}

	Example of classification rule: Assign the new value $x$ to the class with the greatest likelihood
	
	$$\hat{y} = \text{argmax}L(\mu_i, \sigma_i|x)$$	
	
}

\frame{
	\frametitle{Motivation of discriminant analysis} 
	
	\begin{itemize}
		\item DA partitions $X = R^d$ into regions with the same class predictions via separating hyperplanes.
		\item Fit a Gaussian distribution for each class
		\item Find the line where the probability of being in both classes is the same. 
	\end{itemize}
	
	\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ldamotive.png}
	
}

\frame{
\frametitle{Motivation of discriminant analysis} 


\begin{block}{Derive the posterior probability for subject $i$ to belong to class $k$ based on:}
\begin{itemize}
\item  \textbf{Likelihood} of the data for $p=1$ predictor (univariate Gaussian distribution) conditional on class $k$
\begin{equation}
f (x\mid k) = \frac{1}{\sqrt{2\pi} \sigma_k} \exp\left\{ -\frac{1}{2} (\frac{x- \mu_k}{\sigma_k})^2 \right\} \nonumber
\end{equation}
where 
\begin{itemize}
\item  $\mu_k$ is the expectation of $x$ in group $k$ and 
\item $\sigma_k^2$ the variance of $x$ in group $k$.
\item Note that each class $k=0$ or $k=1$  has its specific mean $\mu_k$ and variance $\sigma_k^2$.
\end{itemize}
\item \textbf{Prior probability} of belonging to class $k$ is $\pi_k$
\end{itemize}
\end{block}

}


\frame{
\frametitle{Motivation of discriminant analysis} 


\begin{block}{Derive the posterior probability for subject $i$ to belong to class $k$ based on:}
\begin{itemize}
\item  \textbf{Likelihood} of the data for $p$ predictors (multivariate Gaussian distribution) conditional on class $k$
\begin{equation}
f (x\mid k) = \frac{1}{ (2\pi)^{p/2} \mid \Sigma_k \mid^{1/2}} \exp \left\{ -\frac{1}{2} (x- \mu_k)^t \Sigma^{-1}_k (x - \mu_k) \right\} \nonumber
\end{equation}
where 
\begin{itemize}
\item  $\mu_k$ is the vector of expectations (length $p$) of $x$ in group $k$ and 
\item $\Sigma_k$ the covariance matrix (dimension $p \times p$) of $x$ in group $k$.
\item Note that each class $k=0$ or $k=1$ has its specific mean $\mu_k$ and covariance $\Sigma_k$.
\end{itemize}
\item \textbf{Prior probability} of belonging to class $k$ is $\pi_k$
\end{itemize}
\end{block}

}

\frame{
\frametitle{Note on the multivariate Gaussian distribution} 

\centering \includegraphics [scale=0.33]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/mv-norm.jpeg}


\begin{itemize}
\item Contour plot of two-dimensional Gaussian distribution with the same mean vector but different covariance matrices.
\end{itemize}

}

\frame{
	\frametitle{Note on the multivariate Gaussian distribution} 
	When covariance is zero
	\begin{figure}
	\centering \includegraphics [scale=0.33]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/covzero}
	\end{figure}
	
}

\frame{
	\frametitle{Note on the multivariate Gaussian distribution} 
	When covariance is not zero
	\begin{figure}
	\centering \includegraphics [scale=0.33]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/covnozero}
\end{figure}	
}


\subsection{Bayes' Theorem for classification}

\frame{
\frametitle{Bayes' Theorem for classification}

Probability of being in a class or more formally: posterior probability that $y=k$, where $k=1$ or $k=0$, given the data $x$
\begin{equation}
\pr(y=k \mid x) = \frac{\pi_k f(x\mid k)}{f(x)} \nonumber
\end{equation}
where 
\begin{itemize}
\item $\pi_k$ prior probability for class $k$
\item $f(x\mid k)$ likelihood of $x$ conditional on class $k$
\item $f(x)$ joint distribution defined as mixture 
\begin{equation}
f(x) = \pi_0 f_0(x) + \pi_1 f_1(x) \nonumber
\end{equation}
\end{itemize}

}


\frame{
\frametitle{Prediction rule}

The prediction rule is given by the difference between the two discriminant scores $d_0$ and $d_1$
\begin{eqnarray}
\delta(x_i) & = &  \log(\pr(y=0 \mid x_i)) - \log(\pr(y=1 \mid x_i))  \nonumber \\
			 & = &  d_0(x) - d_1(x)  \nonumber
\end{eqnarray}
where observation $i$ is classified as 
\begin{itemize}
\item $y_i = 1$ if $\delta(x_i)\leq 0$
\item $y_i = 0$ if $\delta(x_i) >  0$
\end{itemize}

}


\subsection{Model specifications}


\frame{
\frametitle{Model specifications and flexibility}

There are three types of discriminant analysis depending on the specification of the covariance $\Sigma_k$: 
\begin{enumerate}
\item \textbf{Diagonal discriminant analysis (dda)}: $\Sigma_k = diag(\sigma^2)$, assuming no covariance between predictors.
\begin{itemize}
\item Parameters to estimate: $p$ diagonal elements
\end{itemize}
\item \textbf{Linear discriminant analysis (lda)}: $\Sigma_k = \Sigma$, assuming covariance between predictors, but it is the same for both groups $k=0$ and $k=1$.
\begin{itemize}
\item Parameters to estimate: full covariance matrix ($p(p+1)/2$ parameters)
\end{itemize}
\item \textbf{Quadratic discriminant analysis (qda)}: $\Sigma_k = \Sigma_k$, assuming covariance between predictors, which may differ between group $k=0$ and $k=1$.
\begin{itemize}
\item Parameters to estimate: twice full covariance matrix ($2 \times p(p+1)/2$ parameters)
\end{itemize}
\end{enumerate}

}



\frame{
\frametitle{Diagonal discriminant analysis}

\begin{itemize}
\item \textbf{dda}: $\Sigma_k = diag(\sigma^2)$, assuming no covariance between predictors.
\item dda is also known as `naive Bayes classifier'.
\item The discriminant function $d_k$ for dda simplifies (after dropping terms which are identical for $d_0$ and $d_1$) to: 
\end{itemize}
\begin{equation}
d_k(x) = \mu_k^t diag(\sigma^2)^{-1} x - \frac{1}{2} \mu_k^t diag(\sigma^2)^{-1} \mu_k + log(\pi_k) \nonumber
\end{equation}

\begin{block}{Advantages of dda}
\begin{itemize}
\item Easy to fit.
\item Strong assumptions (no correlation between predictors and the same covariance matrix in both groups).
\end{itemize}
\end{block}
}


\frame{
\frametitle{Linear discriminant analysis}

\begin{itemize}
\item \textbf{lda}: $\Sigma_k = \Sigma$, allowing for covariance between predictors.
\item The discriminant function $d_k$ for lda simplifies (after dropping terms which are identical for $d_0$ and $d_1$) to: 
\end{itemize}
\begin{equation}
d_k(x) = \mu_k^t \Sigma^{-1} x - \frac{1}{2} \mu_k^t \Sigma^{-1} \mu_k + log(\pi_k) \nonumber
\end{equation}

\begin{block}{Advantages of lda}
\begin{itemize}
\item Allowing for correlation between predictors.
\item Issues for big data ($n<p$) to invert $\Sigma$.
\item Assuming the same covariance matrix $\Sigma$ for group $k=0$ and $k=1$.
\end{itemize}
\end{block}
}

\frame{
	\frametitle{Example LDA} 
	
	\begin{itemize}
		\item Iris dataset: separate the different species based on petal width and length
	\end{itemize}
	
	\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ldaex1.png}	
	
}

\frame{
	\frametitle{Example LDA} 
	
	\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ldaex2.png}	
	
}


\frame{
\frametitle{Quadratic discriminant analysis}

\begin{itemize}
\item \textbf{qda}: $\Sigma_k$, allowing for different covariances between groups.
\item The discriminant function $d_k$ for qda simplifies (after dropping terms which are identical for $d_0$ and $d_1$) to: 
\end{itemize}
\begin{equation}
d_k(x) = \mu_k^t \Sigma^{-1}_k x - \frac{1}{2} \mu_k^t \Sigma^{-1}_k \mu_k + log(\pi_k) \nonumber
\end{equation}

\begin{block}{Advantages of lda}
\begin{itemize}
\item Allowing for correlation between predictors.
\item Issues for big data ($n<p$) to invert $\Sigma$.
\item qda allows for the most flexibility at the cost of many additional parameters.
\end{itemize}
\end{block}
}

\frame{
	\frametitle{QDA Iris} 
	
	\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ladex3.png}	
	
}


\subsection{Linear and quadratic discriminant analysis in \texttt{R}}


\begin{frame}[fragile]{\texttt{lda} in the \texttt{MASS} package: Iris}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
head(iris)

  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa

#fit LDA model
model <- lda(Species~., data=train)
model
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{\texttt{lda} in the \texttt{MASS} package: Iris}
	\begin{lstlisting}[language=R, basicstyle=\scriptsize]
Call:
lda(Species ~ ., data = train)

Prior probabilities of groups:
setosa    versicolor  virginica 
0.3207547  0.3207547  0.3584906 

Group means:
              Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa       -1.0397484   0.8131654   -1.2891006  -1.2570316
versicolor    0.1820921  -0.6038909    0.3403524   0.2208153
virginica     0.9582674  -0.1919146    1.0389776   1.1229172

Coefficients of linear discriminants:
              LD1        LD2
Sepal.Length  0.7922820  0.5294210
Sepal.Width   0.5710586  0.7130743
Petal.Length -4.0762061 -2.7305131
Petal.Width  -2.0602181  2.6326229

\end{lstlisting}
\end{frame}






\frame{
\frametitle{\texttt{lda} in the \texttt{MASS} package}
 
\begin{itemize}
\item Fit lda classification rule: \texttt{lda.out = lda(x, y)}
\includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lda_out}
\item Equivalently, use \texttt{qda.out = qda(x, y)} for qda.
\item Prediction: \texttt{lda.pred = predict(lda.out, x) }
\begin{itemize}
\item lda.pred$\$$class: predicted class
\includegraphics [scale=0.6]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lda_out_class}
%\item lda.pred$\$$posterior: posterior probabilities for group 0 and 1 
%\item lda.pred$\$$x: discriminant score
\end{itemize}
\end{itemize}
}

\frame{
\frametitle{\texttt{lda} in the \texttt{MASS} package}

Prediction: \texttt{lda.pred = predict(lda.out, x) }
\begin{itemize}
\item lda.pred$\$$posterior: posterior probabilities for group 0 and 1 
\includegraphics [scale=0.2]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lda_pp.jpeg}
\item lda.pred$\$$x: discriminant score\\
 \includegraphics [scale=0.04]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gg-dda-score.png}
\end{itemize}

}


\subsection{Shrinkage estimates}

\frame{
\frametitle{Shrinkage estimate}
 
\begin{itemize}
\item Standard implementations of discriminant analysis use the sample covariance to estimate $\Sigma$.
\item The computation of the discriminant function for lda and qda requires the inversion of $\Sigma$, a $p \times p$ matrix.
\item In high-dimensional settings where $n<p$, $\Sigma$ cannot be inverted. 
\item Shrinkage estimates for $\Sigma$ ensure lda and qda can be computed for high-dimensional data. 
\item Implementation \texttt{sda} package in \texttt{R}. 
\end{itemize}

Let me know if you want more references and slides here. 
}

\subsection{Shrinkage discriminant analysis in \texttt{R}}


\frame{
\frametitle{Application: Imaging data to predict breast cancer diagnosis}
 
\begin{itemize}
\item Outcome: Diagnosis (M = malignant, B = benign) \\

\includegraphics [scale=0.8]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/tableY}

\item $n=569$ Breast cancer patients
\item $p=30$ Predictors: 
\begin{itemize}
\item Derived from digitized images to define characteristics of the cell nuclei present in the image.
\item Ten real-valued features summarised in mean, se and worst (mean of the three largest values).
\end{itemize}
\item Dataset from the UCI Machine Learning Repository: \\
\technical
\end{itemize}



}



\frame{
\frametitle{\texttt{sda} package in \texttt{R}}
 
\begin{itemize}
\item dda: sda(x, y, diagonal=TRUE)
\end{itemize}

\centering  \includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/wdbc-dda}

\begin{itemize}
\item \texttt{confusionMatrix} from the \texttt{crossval} package
\end{itemize}

\centering  \includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/wdbc-dda-cM}

}


\frame{
\frametitle{\texttt{sda} package in \texttt{R}}
 
\begin{itemize}
\item lda: sda(x, y, diagonal=FALSE)
\end{itemize}

\centering  \includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/wdbc-lda}

\begin{itemize}
\item \texttt{confusionMatrix} from the \texttt{crossval} package
\end{itemize}

\centering  \includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/wdbc-lda-cM}

}

\frame{
\frametitle{ROC curve}
 
\begin{enumerate}
\item Define foreground (fg) and background (bg): \\
\texttt{fg = dda.pred$\$$posterior[y == 'M',2]} \\
\texttt{bg = dda.pred$\$$posterior[y == 'B',2]} \\
\item Compute the ROC curve: \\
%pr.dda=pr.curve(fg,bg,curve=T)
\texttt{roc.dda = roc.curve(fg,bg,curve=T)} \\
\end{enumerate}

\centering  \includegraphics [scale=0.32]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ROC-wdbc.jpeg}


}


\frame{
\frametitle{Precision-recall curve}
 
\begin{enumerate}
\item Define foreground (fg) and background (bg): \\
\texttt{fg = dda.pred$\$$posterior[y == 'M',2]} \\
\texttt{bg = dda.pred$\$$posterior[y == 'B',2]} \\
\item Compute the ROC curve: \\
\texttt{pr.dda=pr.curve(fg,bg,curve=T)} \\
\end{enumerate}

\centering  \includegraphics [scale=0.32]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/PR-wdbc.jpeg}


}




\section{Support vector machines}
\subsection{Motivation}

\frame{
\frametitle{An introduction to support vector machines}
 
Evolution of support vector machines: 
\begin{enumerate}
\item \textbf{Hyperplanes} to introduce simple decision boundaries.
\item \textbf{Maximal margin classifier} as the `best' hyperplane.
\item \textbf{Support vector classifier} allowing for soft margins. 
\item \textbf{Support vector machine} using kernels for non-linear decision boundaries.
\end{enumerate}

}

\subsection{Hyperplanes}
\frame{
\frametitle{What is a hyperplane?}
 
\begin{itemize}
\item In a $p$-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$. \\
$\rightarrow$ Considering two dimensions, a hyperplane is a line. \\
$\rightarrow$ Considering three dimensions, a hyperplane is a plane.
\item For example in two dimensions, a hyperplane is defined by
 \begin{equation}
\beta_0 + \beta_1 x_1 + \beta_2 x_2= 0 \nonumber
\end{equation}
\item A hyperplane separates observations $x=(x_1, x_2)$ into two groups
\begin{equation}
\beta_0 + \beta_1 x_1 + \beta_2 x_2 \begin{cases} > 0 \\
< 0 \end{cases} \nonumber
\end{equation}
\end{itemize}
}


\frame{
\frametitle{What is a hyperplane?}
 
 \begin{equation}
1 + 2 x_1 + 3 x_2 \begin{cases}
> 0  \text{ blue } \\
< 0  \text{ purple } 
\end{cases} \nonumber
\end{equation}

\centering  \includegraphics [scale=0.28]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/hyperplane}

}



\frame{
\frametitle{What is a hyperplane?}
 
\begin{itemize}
\item If two classes are perfectly separable, there is not a unique solution for a hyperplane.
\end{itemize}
\centering  \includegraphics [scale=0.36]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/hyperplanes2}

}


\subsection{Maximal margin classifier}
\frame{
\frametitle{Maximal margin classifier}
 
\begin{itemize}
\item Maximal margin classifier provide unique analytical solutions.
\item This is the optimal separating hyperplane that has the highest minimum distance (margin)
to the training data.
\end{itemize}
\centering  \includegraphics [scale=0.25]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/hyperplanes3}

\begin{itemize}
\item Note: The maximal margin classifier only depends on the three observations closest to the decision boundary, not to the other observations.
\end{itemize}

}


\subsection{Maximal margin classifier}
\frame{
	\frametitle{Maximal margin classifier}
	\begin{figure}
		\centering  \includegraphics [scale=0.25]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/hyperplanes3}
	\end{figure}

	We need to define
	\begin{enumerate}
		\item Separating hyperplane $\beta_0 + \beta_1X1 + \beta_2X_2$
		\item Margin (distance to the closest data points from hyperplane)
		\item Support vectors (points with distance equal to the margin from hyperplane)
	\end{enumerate}
	
}



\frame{
	\frametitle{Optimisation: Maximal margin classifier}
	
	\begin{flalign}
	& \max_{\beta_0, \beta_1, ..., \beta_p}  M \nonumber \\ 
	& \text{subject to } \sum_{j=1}^p \beta_j^2 =1 \nonumber \text{ forces unique sollution}\\ 
	& y_i (\beta_0 +  \beta_1 x_{i1} +  ... + \beta_p  x_{ip} ) \geq M \quad  \forall i \in 1,...,n \nonumber 
	\end{flalign}
	
	
	\begin{itemize}
		\item $\beta_0, \beta_1, ..., \beta_p$ parameters to be fitted
		\item $i \in 1,...,n$ observations	
		\item $M$ is the margin, distance of observation to decision boundary
		\item Outcome: $y_i = \begin{cases} 1 & \text{ if $i$ in group 1} \\
		-1 & \text{ if $i$ in group 0} \end{cases}  $
	\end{itemize}
	
}


\frame{
	\frametitle{Maximal margin classifier}
	
	\begin{figure}
		\centering  \includegraphics [scale=0.30]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/svmex2}
	\end{figure}

	\begin{itemize}
	\item Good for separating non-Gaussian data
	\item Is not possible if there is no linear separation
\end{itemize}
	
}


\subsection{Support vector classifier}
\frame{
\frametitle{Support vector classifier}
 
\begin{itemize}
\item \textbf{Support vectors}: Observations which support the maximal margin hyperplane.
\item In most application examples, there is no separating hyperplane that can separate two groups.
\end{itemize}

\begin{block}{Aim: Support vector or soft margin classifier}
\begin{itemize}
\item Develop a hyperplane that \textcolor{blue}{almost} separates the classes.
\item Better classification of majority of observations.
\item Greater robustness to individual variation.
\item Avoid overfitting to the training data.
\end{itemize}
\end{block}

}




\frame{
\frametitle{Distinction: Margin and hyperplane}
 
\begin{itemize}
\item \textbf{Hyperplane}: Decision boundary (solid line).
\item \textbf{Margin}: Optimised in maximal margin classifier (dashed line).
\end{itemize}

\centering  \includegraphics [scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/hyperplanes4}


}


\frame{
\frametitle{Optimisation: Support vector classifier}
 
\begin{flalign}
& \max_{\beta_0, \beta_1, ..., \beta_p}  M \quad \text{subject to } \sum_{j=1}^p \beta_j^2 =1 \nonumber \\ 
& y_i (\beta_0 +  \beta_1 x_{i1} +  ... + \beta_p  x_{ip} ) \geq M (1-\epsilon_i) \quad \forall i \in 1,...,n \nonumber  \\
& \text{ where } \epsilon_i \geq 0  \text{ and  } \sum_{i=1}^n \epsilon_i \leq C \nonumber 
\end{flalign}

\begin{itemize}
%\item $M$ is the margin, distance of observation to decision boundary
%\item Outcome: $y_i = \begin{cases} 1 & \text{ if $i$ in group 1} \\
% -1 & \text{ if $i$ in group 0} \end{cases}  $
\item \textbf{Slack variables} allow for a few observations to be on the wrong side of the margin or the hyperplane (missclassification)
\item $C$: Budget for the amount that the margin can be violated. 
\end{itemize}

\begin{block}{Difference to maximal margin classifier:}
\begin{itemize}
\item Regularisation in form of a tuning parameter $C$ and slack variables introduces bias but provides better generalisation.
\end{itemize}
\end{block}

}


\frame{
	\frametitle{Support vector classifier}
	
	\begin{figure}
		\centering  \includegraphics [scale=0.30]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/svmex3}
	\end{figure}
	
}

\subsection{Support vector machines}
\frame{
\frametitle{Support vector machines}
 
\begin{itemize}
\item Support vector classifier assume linear decision boundaries.
\item How can we allow for more flexibility and non-linear decision boundaries?
\end{itemize}

\begin{block}{1st Idea: Include quadratic, cubic, or higher-order
polynomial functions of the predictors.}
\begin{itemize}
\item Add the squared $x_1^2, ... , x_p^2$ or cubic predictors $x_1^3, ... , x_p^3$. 
\end{itemize}
\end{block}

\begin{block}{2nd Idea: Use kernel functions.}
\begin{itemize}
\item The optimization algorithm of the support vector classifier is based on the dot product between two observations.
 \begin{equation}
\langle x_i,x_{i'} \rangle  = \sum_{j=1}^{p} x_{ij} x_{i'j}  \nonumber
\end{equation} 
%\item Replace the dot product with a kernel function $K(x_i,x_{i'})$.
\end{itemize}
\end{block}

%https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d
%https://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e

}



\frame{
\frametitle{Kernel functions}
 
The dot product is also known as linear kernel
 \begin{equation}
K(x_i,x_{i'}) = \sum_{j=1}^{p} x_{ij} x_{i'j}.  \nonumber
\end{equation}

Instead of the linear kernel other kernel functions may be used:
\begin{itemize}
\item  Polynomial kernel: 
\vspace{-0.2cm}
 \begin{equation}
K(x_i,x_{i'}) = (1 + \sum_{j=1}^{p} x_{ij} x_{i'j})^d  \nonumber
\end{equation}
\vspace{-0.4cm}
\item Radial basis: 
\vspace{-0.2cm}
 \begin{equation}
K(x_i,x_{i'}) = exp( -\gamma \sum_{j=1}^{p} (x_{ij}- x_{i'j})^2 )  \nonumber
\end{equation}
\vspace{-0.4cm}
\item Sigmoid kernel: 
\vspace{-0.2cm}
 \begin{equation}
K(x_i,x_{i'}) = \gamma \sum_{j=1}^{p} x_{ij} x_{i'j} + r \nonumber
\end{equation}

\end{itemize}

}

\frame{
	\frametitle{Polynomial kernel of degree 2 example}
	Idea: Convert to higher dimension and separate using a hyperplane
	
	\centering  \includegraphics[width=7cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/polkernel01}
	
}

\frame{
	\frametitle{Polynomial kernel of degree 2 example}
	
	
	\centering  \includegraphics[width=7cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/polkernel1}
	
}

\frame{
	\frametitle{Polynomial kernel of degree 2 example}
	
	
	\centering  \includegraphics[width=7cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/polkernel21}
	
}

\frame{
	\frametitle{Polynomial kernel of degree 2 example}
	
	
	\centering  \includegraphics[width=7cm]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/polkernel4}
	
}

\frame{
\frametitle{Kernel functions}
 

\centering  \includegraphics [scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/svm_kernel}


\begin{itemize}
\item Left: Polynomial kernel of degree 3
\item Right: Radial kernel
\end{itemize}

}

\subsection{Support vector machines in \texttt{R}}

\frame{
\frametitle{\texttt{svm} function in \texttt{R}}
 
\begin{itemize}
\item \texttt{svm} function in the \texttt{library(e1071)}
\end{itemize}

\centering  \includegraphics [scale=0.48]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/svm-R}


}

\frame{
\frametitle{\texttt{svm} function in \texttt{R}}
 

\begin{center}  
\includegraphics [scale=0.48]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/svm-R-out} \\
\end{center}

Options:
\begin{itemize}
\item \texttt{kernel}: Kernel function
\item \texttt{cost}:  Regularisation parameter $C$ or budget
\item \texttt{probability = TRUE}: To output posterior probabilities of r class membership 
\end{itemize}

}





\frame{
\frametitle{Take away: Machine learning: Classification} 

\begin{itemize}
\item Evaluation of classification performance based on parameters derived from the confusion matrix.
\item Visualisation using ROC and precision-recall curves.
\item Discriminant analysis is a parametric approach for classification. 
\item Diagonal, linear and quadratic discriminant analysis are based on different assumptions on the covariance matrix and allow for different flexibility in model fit.  
\item Support vector machines are non-parametric machine-learning type of approaches which offer the most flexibility. 
\item Support vector machines are optimized for prediction and operate in a black box type of implementation (hard to understand mechanistics and interpret).
\end{itemize}

}



\frame{
\frametitle{General background reading} 


\includegraphics [scale=0.30]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ISL2}

\vspace{0.2cm}
 An Introduction to Statistical Learning:  \\
\begin{itemize}
\item 4.4 Linear Discriminant Analysis
\item 9 Support Vector Machines	
\end{itemize}
\url{http://faculty.marshall.usc.edu/gareth-james/ISL/}


}



\frame{
\frametitle{Next lectures} 


\textbf{LECTURE 4b Machine learning: Ensemble methods}
\begin{itemize}
\item Decision trees
\item Ensemble methods
\end{itemize}
\textbf{LECTURE 4c Machine learning: Neural networks}
\begin{itemize}
\item Neural networks
\item Deep learning
\item Machine learning resources in R
\end{itemize}


}




\end{document}







