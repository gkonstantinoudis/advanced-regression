\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}


\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 5 Machine learning: Neural networks}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}



\date{21st March 2023}

\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}


\frame{
\frametitle{Week 5 overview}
	\begin{itemize}
		\item 10-10:50 Neural Networks: Introduction of basic concepts.
		\item 11:00-13:00 Tutorial on random forest: Make sure you all do part 1 and discuss it. 
		\item 14:00-15:00 Mock exam \& Formative assessment: Go through it together. 
		\item 15:00-15:30 Q\&A. Another one 20.04.2023, at 11:00 \url{https://imperial-ac-uk.zoom.us/j/95015633726?pwd=bDNJTVVoL1Z2ajZzWE0rRGFjYSswZz09}.
		\item 15:30-16:00 Presentation on lags. 
		\item Course feedback.
	\end{itemize}

}


\section{Neural networks}
\subsection{Introduction to neural networks}
\frame{
\frametitle{Neural networks}

\begin{itemize}
\item Neural network are a large class of models and learning methods offering great flexibility.
\item Group of units called nodes or neurons that are connected allowing them to
transmit signals between themselves.
\item Inspired by the brain's interconnected network of neurons.
\end{itemize}

\begin{center} 
\includegraphics [width=0.8\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/neuron3.png} 
\end{center}

}


\frame{
\frametitle{Neural network's neurons are organised into layers}


\begin{itemize}
\item \textbf{Input}: Predictors or features 
\item \textbf{Hidden layer}: Neurons transmitting signal 
\item \textbf{Output}: Outcome
\end{itemize}

\begin{center} 
\includegraphics [width=0.66\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/neuralN} 
\end{center}

}


\frame{
	\frametitle{Motivating example: Recognise handwritten digits}
	\begin{itemize}
		\item Online book by Michael Nielsen (\url{http://neuralnetworksanddeeplearning.com/chap1.html})
	\end{itemize}
	\begin{center} 
		\includegraphics [width=0.66\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/handunits} 
	\end{center}
	
}


\frame{
	\frametitle{Motivating example: Recognise handwritten digits}
	\begin{itemize}
		\item \url{https://www.3blue1brown.com/lessons/neural-networks}
		\item \url{https://www.youtube.com/watch?v=aircAruvnKk}
	\end{itemize}
	\begin{center} 
		\includegraphics [width=0.70\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/id9} 
	\end{center}
	
}


\frame{
	\frametitle{Motivating example: Recognise handwritten digits}
	\begin{center} 
		\includegraphics [width=0.80\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/nn4} 
	\end{center}
	
}




\subsection{Neural network model formulation}

\frame{
\frametitle{Neural networks}

\begin{block}{$K$-class classification task:}
\begin{itemize}
\item $y_k$, where  $k = 1,...,K$  each being coded as a binary dummy variable for the kth class.
\end{itemize}
\end{block}

\begin{itemize}
\item \textbf{Input}: $p$ predictors or features $x_1, ..., x_p$ 
\item \textbf{Hidden layer}: $M$ derived features $h_1, ..., h_M$ 
\begin{equation}	
h_m = \sigma(\alpha_{0,m} + \alpha_m^t x), \text{ for } m \in 1,...,M \nonumber
\end{equation}
\item \textbf{Targets}: $K$ predictors
\begin{equation}	
t_k = (\beta_{0,k} + \beta_k^t h), \text{ for } k \in 1,...,K \nonumber
\end{equation}
\item \textbf{Output}: $K$ predictions $f_k (x)$
\begin{equation}	
f_k (x) = g_k (t), \text{ for } k \in 1,...,K \nonumber
\end{equation}
\end{itemize}


}



\frame{
\frametitle{Hidden layer: $M$ derived features $h_1, ..., h_M$}

%\textbf{Hidden layer}: $M$ derived features $h_1, ..., h_m$ 
\begin{equation}	
h_m = \sigma(\alpha_{0,m} + \alpha_m^t x), \text{ for } m \in 1,...,M \nonumber
\end{equation}
\vspace{-0.2cm}
\begin{itemize}	
\item $\alpha_{0,m}$ offset or intercept for $m$th hidden layer 
\item $\alpha_m$ weights (vector of length $p$) of the input features 
\item $\sigma$ activation functions of $s=\alpha_{0,m} + \alpha_m^t x$:
\begin{itemize}
\item[$\diamond$] Linear $f(s)=s$	
\item[$\diamond$] Sigmoid
\begin{equation}	
f(s) = \frac{1}{1+\exp(-c \times s)} \nonumber
\end{equation}
\item[$\diamond$] Relu
\begin{equation}	
f(s) = \begin{cases} s \text{ if } s\geq 0  \nonumber \\
0 \text{ if } s<0  \nonumber \end{cases}
\end{equation}
\item[$\diamond$] Hyperbolic tangent (Tanh)
\begin{equation}	
f(s) = \frac{\exp(s) - \exp(-s)}{\exp(s) + \exp(-s)} \nonumber
\end{equation}
\end{itemize}
\end{itemize}


}



\frame{
\frametitle{Activation function}

%\textbf{Hidden layer}: $M$ derived features $h_1, ..., h_m$ 
Sigmoid function
\begin{equation}	
f(s) = \frac{1}{1+\exp(-c \times s)} \nonumber
\end{equation}
\begin{itemize}	
\item $c$ scale parameter
\end{itemize}

\vspace{-0.1cm}

\begin{center} 
\includegraphics [width=0.8\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/sigmoid} 
\end{center}


}



\frame{
\frametitle{Prediction function $f_k(x)$}

%\textbf{Hidden layer}: $M$ derived features $h_1, ..., h_m$ 
\begin{equation}	
f_k (x) = g_k (t), \; k \in 1,...,K \nonumber
\end{equation}
where
\begin{itemize}
\item Targets are defined as 
\begin{equation}	
t_k = (\beta_{0,k} + \beta_k^t h), \text{ for } k \in 1,...,K \nonumber
\end{equation}
\item $\beta_{0,k}$ offset or intercept for $k$th target 
\item $\beta_k$ weights (vector of length $m$) of the input features 
\item $g_k$ output function
\begin{itemize}
\item[$\diamond$] Identity function
\item[$\diamond$] Softmax
\begin{equation}	
g_k(t) = \frac{\exp t_k}{\sum_{k=1}^K \exp t_k} \nonumber
\end{equation}
\end{itemize}
\end{itemize}


}






\frame{
\frametitle{Training the neural network}

\begin{itemize}
\item Feed-forward perceptron which uses only a one hidden layer.
\item Parameters to fit:
\begin{eqnarray}
\{\alpha_{0,m}, \alpha_m; m\in 1,...M\}; &  M(p+1) & \text{ feature weights}  \nonumber \\
\{\beta_{0,k}, \beta_k; k\in 1,...K\}; &  K(M+1) & \text{ prediction weights} \nonumber
\end{eqnarray}
\item Back-propagation algorithm using gradient descent and the chain rule.
\item For full details on the algorithm see section 11.4 \\ in Elements of Statistical Learning
\url{https://web.stanford.edu/~hastie/Papers/ESLII.pdf}
\end{itemize}


}



\frame{
\frametitle{Issues in neural networks}

\begin{itemize}
\item Starting values of the algorithm
\item Features need to be scaled prior to the analysis
\item Choice of the number of hidden units and layers \\
Increasing the amount of hidden layers and units increases the complexity
and ability to model more complicated non-linear patterns.
\item Overfitting
\end{itemize}

Regularisation with additional parameters:
\begin{itemize}
\item Decay parameter: Penalty for large weights 
\item Drop out: Random dropping out of nodes during training to prevent overfitting
\item Learning rate: Scaling the magnitude of the weight updates
\end{itemize}

}


\subsection{\texttt{neuralnet} function in \texttt{R}}


\frame{
\frametitle{\texttt{neuralnet} function in the \texttt{neuralnet} package} 

\texttt{neuralnet(f,data=data,hidden,act.fct,linear.output=T)}
\begin{itemize}
\item \texttt{f}: formula
\item \texttt{data}: input data
\item \texttt{hidden}: specification of the hidden layer structure
\item \texttt{act.fct}: activation function
\item \texttt{linear.output=T}: linear activation function
\end{itemize}

}


\frame{
\frametitle{\texttt{neuralnet} function in the \texttt{neuralnet} package} 

\texttt{f}: formula
\begin{itemize}
\item \texttt{y $\sim$ x1 + x2 + x3}
\end{itemize}

\texttt{hidden}: specification of the hidden layer structure
\begin{itemize}
\item Length represents the number of layers and the 
\item Numbers represent the number of units
\item Examples: 
\begin{itemize}
\item[$\diamond$] \texttt{hidden = c(5)}: 1 layer with 5 units
\item[$\diamond$] \texttt{hidden = c(5,2)}: 2 layers with 5 and 2 units 
\end{itemize}
\end{itemize}

Functionalities:
\begin{itemize}
\item  \texttt{plot.nn}: Plot the neural network.
\item \texttt{predict.nn}: Prediction of outcome based on new input data.
\end{itemize}

}



\frame{
\frametitle{Application example \texttt{neuralnet}} 

\begin{enumerate}
\item  Write the formula: \\
\texttt{f = as.formula(paste("y $\sim$", paste(colnames(x), collapse = "+")))}
\item Fit the neural network with one layer \\
\texttt{nn1 = neuralnet(f,data=data,hidden=c(5),linear.output=T)}
\item Fit the neural network with two layers \\
\texttt{nn2 = neuralnet(f,data=data,hidden=c(5,3),linear.output=T)}
\item Plot the neural networks 
\begin{itemize}
\item[$\diamond$] plot(nn1)
\item[$\diamond$] plot(nn2)
\end{itemize}
\end{enumerate}

}


\frame{
\frametitle{Example \texttt{neuralnet}: One layer with 5 units} 



\begin{center} 
\includegraphics [width=0.7\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/nn1} 
\end{center}

}



\frame{
\frametitle{Example \texttt{neuralnet}: Two layers with 5 and 3 units} 


\begin{center} 
\includegraphics [width=0.7\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/nn2} 
\end{center}

}




\section{Deep learning}

\subsection{Deep learning with the \texttt{keras} package}

\frame{
	\frametitle{Deep learning in \texttt{R}} 
	
	\begin{block}{Deep learning algorithms}
		Are neural networks with multiple layers, a deep structure.
	\end{block}
	
	\begin{itemize}
		\item \texttt{tensorflow} package \\
		Interface to TensorFlow \url{https://www.tensorflow.org}, an open source software library for numerical computation using data flow graphs. TensorFlow was  developed by the Google Brain Team within Google's Machine Intelligence research organization.
		\item \texttt{keras} package \\
		Interface to keras \url{https://keras.io} a high-level neural networks application programming interface (API) based on python.
	\end{itemize}
	
}

\frame{
	\frametitle{$k$ class classification in \texttt{keras}} 
	
	\begin{itemize}
		\item Extensive tutorials online:
		\url{https://cloud.r-project.org/web/packages/keras/index.html}
		\item Application example from zalando research
		\url{https://cran.r-project.org/web/packages/keras/vignettes/tutorial_basic_classification.html}
		\item Aim: Predict the product category ($k=10$ class classification)
		\vspace{-0.2cm}
		\begin{enumerate}
			\item[0.] T-shirt/top
			\item[1.] Trouser
			\item[2.] Pullover
			\item[3.] Dress
			\item[4.] Coat
			\item[5.] Sandal
			\item[6.] Shirt
			\item[7.] Sneaker
			\item[8.] Bag
			\item[9.] Ankle boot
		\end{enumerate}
	\end{itemize}
	
}


\frame{
	\frametitle{Input} 
	
	\begin{itemize}
		\item 70,000 grayscale images at low resolution of 28 by 28 pixels
	\end{itemize}
	
	\begin{center} 
		\includegraphics [width=0.55\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion} 
	\end{center}
	
}



\frame{
	\frametitle{Data structure} 
	
	\begin{itemize}
		\item Training data: 60,000 images 
		\begin{center} 
			\includegraphics [width=0.6\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion_train} 
		\end{center}
		\item Test data: 10,000 images
		\begin{center} 
			\includegraphics [width=0.55\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion_test} 
		\end{center}
	\end{itemize}
	
	\begin{center} 
		\includegraphics [width=0.34\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion1}
		\includegraphics [width=0.34\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion6}
	\end{center}
	
	
}




\frame{
	\frametitle{Building the model} 
	
	\begin{enumerate}
		\item \textbf{Specify the model}: \\
		\small{\texttt{model = keras$\_$model$\_$sequential() \\
				model $\%>\%$ \\
				$\:$ layer$\_$mflatten(input$\_$shape = c(28, 28)) $\%>\%$ \\
				$\:$  layer$\_$mdense(units = 128, activation = 'relu') $\%>\%$ \\
				$\:$  layer$\_$mdense(units = 10, activation = 'softmax')}} 
		\item \textbf{Compile the model}: \\
		\small{\texttt{model $\%>\%$ compile( \\
				$\:$  optimizer = 'adam',  \\
				$\:$  loss = 'sparse$\_$categorical$\_$crossentropy', \\
				$\:$  metrics = c('accuracy') \\
				)}}
		\item \textbf{Fit the model}: \\
		\small{\texttt{model $\%>\%$ fit(train$\_$images, train$\_$labels, epochs = 5)}}
	\end{enumerate}
	
}


\frame{
	%\frametitle{Specify and compile the model} 
	
	\textbf{Specify the neural network structure}
	%\texttt{layer$\_$mdense(units = 128, activation = 'relu')}
	\begin{itemize}
		\item \texttt{layer$\_$mdense}: Fully connected (dense) hidden layer 
		\item \texttt{units}: Number of units
		\item \texttt{activation}: Activation function (exponential, elu, relu, softmax, sigmoid, selu, softplus, tanh and many more)
		\item Use multiple $layer$\_$mdense$ to add more layers
	\end{itemize}
	%\texttt{model $\%>\%$ compile}
	\textbf{Compile the model}
	\begin{itemize}
		\item \texttt{optimizer}: Adam, adagrad, adadelta, adamax stochastic gradient descent (SGD) and others
		\item \texttt{loss}: Depending on task (regression:  mean$\_$squared$\_$error, mean$\_$absolute$\_$error, huber$\_$loss, kullback$\_$leibler$\_$divergence; classification: categorical$\_$crossentropy, sparse$\_$categorical$\_$crossentropy and many more)
		\item \texttt{metrics}: Depending on task (regression: mean$\_$squared$\_$error, mean$\_$absolute$\_$error; classification: accuracy, binary$\_$accuracy, categorical$\_$accuracy, sparse$\_$categorical$\_$accuracy)
	\end{itemize}
	
	
}



\frame{
	\frametitle{Evaluating training and test error} 
	
	\begin{itemize}
		\item \textbf{Training error}:\\
		\texttt{score = model $\%>\%$ evaluate(train$\_$images, train$\_$labels) \\
			cat('Training loss:', score$\$$loss)\\
			Training loss: 0.2772723 \\
			cat('Training accuracy:', score$\$$acc) \\
			Training accuracy: 0.89925} \\
		
		\item \textbf{Test error}: \\
		\texttt{score = model $\%>\%$ evaluate(test$\_$images, test$\_$labels) \\
			cat('Test loss:', score$\$$loss) \\
			Test loss: 0.3530962 \\
			cat('Test accuracy:', score$\$$acc) \\
			Test accuracy: 0.8742 }
	\end{itemize}
	
}

\frame{
	\frametitle{Making predictions} 
	
	\begin{itemize}
		\item Probabilities for class $k$:
		\texttt{predictions = model $\%>\%$ predict(test$\_$images)}
		\item Class:
		\texttt{class$\_$pred = model $\%>\%$ predict$\_$classes(test$\_$images)}
	\end{itemize}
	
	\begin{center} 
		\includegraphics [width=0.98\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion_out}
	\end{center}
	
}



\frame{
	%\frametitle{Making predictions} 
	\vspace{-0.1cm}
	\begin{center} 
		\includegraphics [width=0.75\textwidth]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/fashion_predict}
	\end{center}
	
	
}



\section{Machine learning resources in \texttt{R}}

\frame{
	\frametitle{Machine learning resources in \texttt{R}} 
	
	CRAN Task view \url{https://cran.r-project.org/web/views/MachineLearning.html} presents a collection of established and curated \texttt{R}-packages on 
	\begin{itemize}
		\item Neural Networks and Deep Learning
		\item Recursive Partitioning
		\item Random Forests
		\item Regularized and Shrinkage Methods
		\item Boosting and Gradient Descent
		\item Support Vector Machines and Kernel Methods
		\item Bayesian Methods
		\item Optimization using Genetic Algorithms
	\end{itemize}
	
}



\frame{
	\frametitle{Take away: Machine learning: Neural networks and deep learning} 
	
	
	\begin{itemize}
		\item Neural networks offer a very flexible and powerful  framework for prediction.
		\item Tuning and optimising a neural network involves many parameters (for example number of layers and units, regularisation) and needs to be performed very carefully.
		\item There are different algorithms for optimisation.
		\item Neural networks are black boxes which optimise prediction but tell us little about the structure of the data.
	\end{itemize}
	
}




\frame{
	\frametitle{Neural network book} 
	
	
	\includegraphics [scale=0.34]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gurney}
	
	\vspace{0.2cm}
	\begin{itemize}
		\item Chapters 1-3 
		\item \url{https://www.inf.ed.ac.uk/teaching/courses/nlu/assets/reading/Gurney_et_al.pdf}
	\end{itemize}
	
}






\frame{
	\frametitle{General background reading\\ The Elements of
		Statistical Learning} 
	
	
	\includegraphics [scale=0.36]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/ESL}
	
	\vspace{0.2cm}
	\begin{itemize}
		\item Chapter 11 
		\item \url{https://web.stanford.edu/~hastie/ElemStatLearn/}
	\end{itemize}
	
}








\end{document}







