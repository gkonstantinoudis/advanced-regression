\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}




\newcommand{\nc}{\newcommand}
\nc{\pr}{\ensuremath{\mathbb{P}}}
\nc{\logit}{\rm{logit}}

\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 1b Linear and generalised linear models (Part I)}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}




\date{21st February 2023}


\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}

%\section{Background: The linear model}
%\section{Diagnostics in the linear model}
%\section{Prediction using linear models}
%\section{Linear models in R}
%\section{Generalised linear model}
%\section{Generalised linear models in R}



\section{The linear model}

\subsection{Basic definition}

\frame{
\frametitle{Main aim} 

Regression models are used to investigate association between 
\begin{itemize}
\item an outcome variable $y$
\item potential explanatory variables (or predictors) $x=(x_1, x_2, \ldots, x_p)$
\end{itemize}

\begin{block}{}{
The statistical idea is to see if the $x_1, x_2, \ldots, x_p$ can give an adequate description of the variability of the
outcome $y$.
}
\end{block}


\centering \includegraphics [scale=0.25]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/diabetes_lm}


}



\frame{
\frametitle{Motivations} 

\begin{block}{}{

\begin{enumerate}
\item \textbf{Understand} how the predictors affect the outcome. 
\begin{itemize}
\item
Example: We conduct an observational study focusing on type 2 diabetes as outcome. Our aim is to understand which risk factors are associated with the risk of type 2 diabetes.
\end{itemize}
\item \textbf{Predict} the outcome of new observations, where only the predictors are observed, but not the outcome. \begin{itemize}
\item
Example: We study type 2 diabetes and want to predict disease progression. Our aim is to identify individuals with poor prognosis and improve their treatment. 
\end{itemize}
\end{enumerate}
}
\end{block}



}



\frame{
\frametitle{The linear model} 

\begin{equation}
y = \alpha +  x \beta + \epsilon \nonumber
\end{equation}

\begin{itemize}
\item $y$: Outcome, response, dependent variable \\
Dimension: $n \times 1$
\item $x$: Regressors, exposures, covariates, input, explanatory, or independent variables \\
Dimension: $n \times p$
\item $\epsilon$: Residuals, error
\item $\alpha$: Intercept 
\item $\beta$: Regression coefficients 
\end{itemize}


}



\frame{
\frametitle{Parameters to estimate:} 
\begin{itemize}
\item $\alpha$: intercept \\
Baseline level, the expected mean value of $y$ when all $x=0$
\item $\beta=(\beta_1,...,\beta_p)$: vector of regression coefficients
\item $\beta_j$: regression coefficients of variable $x_j$ \\
The expected change in $y$ for a one-unit change in $x_j$ when the other covariates are held constant. 
\end{itemize}

Observed data:
\begin{itemize}
\item $y$: Outcome or response
\item $x$: Regressors, exposures, covariates, input, explanatory or independent variables
\begin{itemize}
\item[$\diamond$] $i=1,...,n$ samples
\item[$\diamond$] $j=1,...,p$ variables
 \end{itemize}
\end{itemize}
}



\frame{
\frametitle{Estimates: Ordinary least squares (OLS)} 

\begin{equation}
\hat{\beta}_{OLS} = \underbrace{(x^t x)^{(-1)} }_{p \times p}  \underbrace{x^t}_{p \times n} \underbrace{y}_{n \times 1} \nonumber
\end{equation}

\begin{itemize}
\item Inversion of  $\underbrace{(x^t x)^{(-1)} }_{p \times p}$ requires $x^t x$ to be of full rank \\ (Lecture 2b).
\item Alternative representation:
\begin{equation}
\hat{\beta} = \frac{\hat{cov}(x,y)}{\hat{cov}(x)} \nonumber
\end{equation}
where the sample covariance is defined as:
\begin{itemize}
\item $\hat{cov}(x,y)=\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) $
\item $\hat{cov}(x)=\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})$
\end{itemize}
\end{itemize}

}



\frame{
\frametitle{Example: Diabetes data} 
\begin{itemize}
\item $y$: quantitative measure of disease progression one year after baseline (vector)
\item $x$: predictor matrix
\begin{itemize}
\item[$\diamond$] clinical parameters: age, sex, bmi
\item[$\diamond$] map: blood pressure
\item[$\diamond$] tc: total cholesterol 
\item[$\diamond$] ldl: low-density lipoprotein
\item[$\diamond$] hdl: high-density lipoprotein
\item[$\diamond$] tch: total cholesterol over hdl
\item[$\diamond$] ltg: triglycerides
\item[$\diamond$] glu: glucose
\end{itemize}
\item $n=442$: sample size
\end{itemize}

}


\frame{
\frametitle{The \texttt{lm()} command in R} 
\begin{itemize}
\item \texttt{lm(y $\sim$ age+sex+glu+map+ltg, data = x)}
\item Formula \texttt{y $\sim$ x1+x2+x3} 
\begin{itemize}
\item[$\diamond$] left of $\sim$: outcome
\item[$\diamond$] right of $\sim$: predictors
\end{itemize}
\item It is also possible to enter a full matrix $x$ (transform by \texttt{as.matrix}) as multivariable set of predictors:
\texttt{y $\sim$ x}
\item An intercept is always included, to turn off add \texttt{-1}
\end{itemize}


}




\frame{
\frametitle{Interpreting the summary.lm() command} 
\begin{itemize}
\item \texttt{summary.lm(lm(y $\sim$ age+sex+glu+map+ltg, data = x))}
\end{itemize}

\centering \includegraphics [scale=0.48]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/summary_diabetes}


}


\frame{
\frametitle{Difference between univariable and multivariable regression} 
\begin{itemize}
\item \texttt{summary.lm(lm(y $\sim$glu))}
\end{itemize}

\centering \includegraphics [scale=0.48]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/summary_diabetes2}

\begin{itemize}
\item Reduction  of the regression coefficient from $619$ to $175$ after conditioning on other covariates.  $\rightarrow$ attenuation of the effect.
\end{itemize}
}




\frame{
\frametitle{Further estimates} 
\begin{itemize}
\item  Weighted least squares \\
\begin{equation}
\hat{\beta}_{WLS} = \underbrace{(x^t w x)^{(-1)} }_{p \times p}  \underbrace{x^t}_{p \times n}  \underbrace{w}_{n \times n}  \underbrace{y}_{n \times 1} \nonumber
\end{equation}
where $w$ is a $n \times n$ diagonal weight matrix
\item  Maximum likelihood
\item  Bayesian linear regression (Module: Bayesian Statistics)
\end{itemize}

}



\frame{
\frametitle{Fitted values and residuals} 
\begin{itemize}
\item Fitted values
\begin{equation}
\hat{y} = x \hat{\beta} = \underbrace{x (x^t x)^{-1} x^t}_{h} y \nonumber
\end{equation}
\item Hat matrix $h$
\item Residuals are the difference between the fitted values (predicted by the model) and the actual observed outcome. 
\begin{equation}
r_i = \hat{y}_i - y_i \nonumber
\end{equation}
\item The residuals are a vector $r=(r_1,...,r_n)$ of length $n$.
\end{itemize}


\begin{block}{}{
Residuals are an important quantity for model diagnostics.
}
\end{block}
}

\frame{
\frametitle{Fitted values and residuals} 
\centering \includegraphics [scale=0.5]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/residual}
}



\frame{
\frametitle{lm(): Fitted values and residuals} 
\begin{itemize}
\item First fit a linear model and save it in the object lm0 \\
\texttt{lm0 = lm(y$\sim$x)}
\item The linear model object lm0 contains 
\begin{itemize}
\item[$\diamond$]  Regression coefficients: \texttt{lm0$\$$coefficients}
\includegraphics [scale=0.6]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/coef}
\item[$\diamond$]  Fitted values: \texttt{lm0$\$$fitted.values}
\item[$\diamond$]  Residuals: \texttt{lm0$\$$residuals}
\end{itemize}
\end{itemize}
}





\subsection{Assumptions}






\frame{
\frametitle{Assumptions} 
\begin{enumerate}
\item[I.] Linearity: There is a linear relationship between $x$ and $y$
\item[II.] Weak exogeneity: The predictors $x$ are viewed as fixed variables; there is no measurement error on $x$. 
\item [III.] Constant variance (homoscedasticity): All residuals have the same variance.
\item [IV.] No perfect multicollinearity: No predictor can be expressed as a linear combination of the other predictors (Lecture 2b).
\item [V.] Independent errors: The residuals are uncorrelated 
(e.g. in time-series the error of time point $t$ will depend on the error of time point $t-1$) and independent of $x$.
\end{enumerate}

}

\frame{
\frametitle{Further assumptions} 
\begin{itemize}
\item Normal-distributed errors: \\ The residuals are normal-distributed. \\
Note: This is not required for the OLS estimate, but for the Maximum Likelihood estimation.
\item Outlier: observation point that is distant from other observations. \\
It is recommended to check the data for outliers, which can arise because of many reasons:
\begin{itemize}
\item Measurement error (remove)
\item Errors in the pre-processing steps (fix or remove)
\item `True' biological outliers (follow-up) 
\end{itemize}
\item Influential variants: Cook's distance 
\end{itemize}

}


\subsection{Diagnostic plots in the linear model}

\frame{
\frametitle{Diagnostic plots: Linear relationship} 

\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm0-S0}
\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm4-S0}

\begin{itemize}
\item Scatterplot of $y$ against $x$ 
\end{itemize}



}


\frame{
\frametitle{Diagnostic plots: Linear relationship} 

\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm0-RF1}
\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm4-RF1}

\begin{itemize}
\item Scatterplot of residuals ($y$-axis) against fitted values ($x$-axis)
\end{itemize}
}


\frame{
\frametitle{Diagnostic plots: Homoscedasticity} 


\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm0-SL3}
\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm1-SL3}

\begin{itemize}
\item Scatterplot of standardised residuals ($y$-axis) against fitted values ($x$-axis)
\end{itemize}
}





\frame{
\frametitle{Diagnostic plots: Normal-distribution of residuals} 


\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm0-QQ2}
\includegraphics[scale=0.31]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm2-QQ2}

\begin{itemize}
\item Q-Q plots of observed residuals ($y$-axis) against theoretical values under the Normal distribution ($x$-axis)
\end{itemize}
}

\frame{
\frametitle{Diagnostic plots: Outliers} 


\includegraphics[scale=0.29]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm0-CD4}
\includegraphics[scale=0.29]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/lm3-CD4}

\begin{itemize}
\item Scatterplot of standardised residuals ($y$-axis) against Cook's distance ($x$-axis)
\item Cook's distance measures the effect of deleting a given observation ( sum of all the changes in the regression model when observation $i$ is removed)
\end{itemize}
}


\frame{
\frametitle{lm(): Diagnostics} 
\begin{itemize}
\item Linear relationship and outliers (Scatterplot of y against x) \\
\texttt{plot(x,y) \\
abline(lm0, col=`red')}
\item Linear relationship and outliers (Residuals against fitted values) \\
\texttt{plot(lm0, which=1)}
\item Homoscedasticity (Standardised residuals against fitted values) \\
\texttt{plot(lm0, which=3)}
\item Normal-distribution of residuals (Q-Q plots of observed residuals against theoretical values under the Normal distribution) \\
\texttt{plot(lm0, which=2)}
\item Influential variants: (Standardised residuals against Cook's distance) \\
\texttt{plot(lm0, which=5)}
\end{itemize}

}







\subsection{Prediction using linear models}



\frame{
\frametitle{Prediction using linear models} 
\begin{enumerate}
\item Assume we have a database with $n$ type 2 diabetes cases, where we have measured the following data:
\begin{itemize}
\item $y$: quantitative measure of disease progression one year after baseline (vector)
\item $x$: predictor matrix including clinical data (age, sex, bmi), blood pressure and triglycerides
\end{itemize}
This is our training data $y\_{\text{train}}$ and $x\_{\text{train}}$.
\item For a new case we only have the predictor matrix $x_{\text{new}}$, but not $y_{\text{new}}$.
\item Goal: For each new type 2 diabetes case we want to predict  $y_{\text{new}}$, his/her progression one year later.
\end{enumerate}

}




\frame{
\frametitle{lm(): Predictions} 
\begin{enumerate}
\item Use the linear model to learn a prediction rule from the training data, where both $x$ and $y$ are observed on the same individuals. \\
\texttt{lm$\_$train = lm(formula=y$\_$train$\sim$age+sex+bmi+map+ltg, data=x$\_$train)}
\item Predict the outcome based on the prediction rule and the predictors of the new data. \\
\texttt{predict.lm(lm$\_$train,x$\_$new)}
\end{enumerate}

}






\frame{
\frametitle{Take away: Linear models} 


\begin{itemize}
\item Motivation why to use linear models \\
 (To understand and to predict)
\item Model fit using ordinary least squares
\item Interpretation of the regression coefficients
\item Residuals and fitted values 
\item Model diagnostics
\item Using the linear model to predict
\end{itemize}

}








\end{document}




