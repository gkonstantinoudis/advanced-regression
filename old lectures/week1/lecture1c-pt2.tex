\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}


\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 1c Random effects and hierarchical models (Part II)}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}




\date{21st February 2023}




\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}




\section{Random effect analysis}


\subsection{Definition of random effects}


\frame{
\frametitle{4. Random effects } 

\begin{enumerate}
\item Random effect model with random intercept
\begin{equation}
y_i = (\alpha_0 + {\color{red}{u_k}}) + \beta x_i  + \epsilon_i, \nonumber 
\end{equation}
where $\color{red}{u_k \sim N(0, \sigma_u^2)}$
\item Random effects model on both, the intercept and the slope
\begin{equation}
y_i = (\alpha_0 + {\color{red}{u_k}}) + (\beta + {\color{red}{w_k}}) x_i  + \epsilon_i \nonumber 
\end{equation}
where $\color{red}{w_k \sim N(0, \sigma_w^2)}$
\end{enumerate}



\begin{block}{Group effects are random variables, also called random effects.}{
\begin{enumerate}
\item Random effect for the  intercept  $u_k \sim N(0, \sigma_u^2)$
\item Random effect for the slope $w_k \sim N(0, \sigma_w^2)$
\end{enumerate}
}
\end{block}

}

\subsection{Random effect model with random intercept}

\frame{
\frametitle{Random intercept} 

\begin{enumerate}
\item Random effect model with random intercept
\begin{eqnarray}
y_i &=& (\alpha_0 + u_k) + \beta x_i  + \epsilon_i, \nonumber \\
    &=& \alpha_0 + \beta x_i  + u_k + \epsilon_i, \nonumber 
\end{eqnarray}
\end{enumerate}


\begin{itemize}
\item Where $\alpha_0$ is the intercept and $\beta$ the regression coefficient.
\item There are two distinct error terms
\begin{enumerate}
\item Group-specific error
\begin{equation}
u_k \sim N(0,\sigma_u^2) \nonumber 
\end{equation}
\item Individual-specific error
\begin{equation}
\epsilon_i \sim N(0,\sigma^2) \nonumber 
\end{equation}
\end{enumerate}
\item Note that $u_k$ and $\epsilon_i$ are independent of each other.
\end{itemize}

}


\frame{
\frametitle{Random effect model with random intercept} 

Interpretation of random intercept $\alpha_k$:
\begin{equation}
\alpha_k = (\alpha_0 + u_k)  \nonumber 
\end{equation}
\begin{itemize}
\item $\alpha_0$ is the global intercept
\item $u_k$ group-level variations around the global intercept
\end{itemize}
This is equivalent to assuming $\alpha_k$ is a \textbf{random variable} that follows a Normal distribution
\begin{equation}
\alpha_k \sim N(\alpha_0, \sigma_u^2)  \nonumber 
\end{equation}

}


\frame{
\frametitle{Random effect model with random intercept} 

Multi-level interpretation (two levels of variability):
\begin{enumerate}
\item\underline{First level} \\
Defined on the individual level for observation $i = 1,...,n$, similar to a standard linear regression
\begin{equation}
y_i = \alpha_k + \beta x_i  + \epsilon_i, \nonumber 
\end{equation}
\item \underline{Second level} \\
But the intercept is not fixed, it is a random variable 
\begin{equation}
\alpha_k \sim N(\alpha_0, \sigma_u^2)  \nonumber 
\end{equation}
\end{enumerate}


}


\frame{
\frametitle{Random effect model with random intercept} 

\underline{Assumptions}
\begin{itemize}
\item Slope of regression line is the same across all groups. Each group has a different intercept ($\alpha_k$).
\item But $\alpha_k \sim N(\alpha_0, \sigma_u^2)$ has now a common distribution which is estimated from \textbf{all observations}, and not just from the observations in a specific group as in fixed effects.
\item We pool information across groups.
\end{itemize}
\underline{Consequences}
\begin{itemize}
\item We control for group characteristics by including the group-specific intercept.
\item Number of group-specific parameters to estimate is much smaller than in the fixed effect models ($\sigma_u^2$ vs $k$ intercepts). 
\end{itemize}

}






\subsection{Estimation using Maximum Likelihood}

\frame{
\frametitle{(Restricted) Maximum Likelihood estimation of random effect} 
\begin{equation}
y_i =  \alpha_0 + \beta x_i  + u_k + \epsilon_i, \nonumber 
\end{equation}
Parameters to estimate are
\begin{itemize}
\item $\alpha_0, \beta$ intercept and regression coefficient
\item $\sigma_u^2, \sigma^2$ variance components
\end{itemize}
Maximum Likelihood estimation is based on the Normal distribution of $u_k$ and $\epsilon_i$
\begin{itemize}
\item ML estimate for $\sigma_u^2$ requires subtracting 2 empirical estimates of variance $\rightarrow$ ML estimates for $\sigma_u^2$  can be negative.
\item Restricted Maxium Likelihood (REML): Imposes positivity constraints on the variance estimates.
\end{itemize}

}




\subsection{Random effects in R: lme}


\frame{
\frametitle{Random intercept in \texttt{R}} 


Implementations of Restricted Maximum Likelihood (REML) in \texttt{R}
\begin{itemize}
\item \texttt{lmer} function in the \texttt{lme4} package
\item \texttt{lme} function in the \texttt{nlme} package
\end{itemize}


\begin{block}{Focus here is the \texttt{lme} function in the \texttt{nlme} package.}{
\texttt{lme(fixed, data, random)}
}
\begin{itemize}
\item \texttt{fixed}: Formula $y\sim x$
\item \texttt{random}: Formula $\sim 1 \mid factor$
\item \texttt{data}: Dataset to use
\end{itemize}

\end{block}


}

\frame{
\frametitle{R: Random intercept using \texttt{lme}} 

\texttt{RandomIntercept = lme( chol $\sim$ age, random = $\sim$ 1 $\mid$ doctor, data = data.chol)  \\
summary(RandomIntercept)}

\vspace{0.2cm}
\centering \includegraphics[scale=0.42]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/summaryRI}



}

\frame{
\frametitle{R: Random intercept using \texttt{lme}} 

\texttt{RandomInterceptPredictions = fitted(RandomIntercept)}

\vspace{0.4cm}
\centering \includegraphics[scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-ggRandomIntercept}



}



\subsection{Variance partition}

\frame{
\frametitle{Random effect model and variance partition} 

Variance decomposition for observation $i$ in group $k$
\begin{eqnarray}
var(y_i) &=& var(u_k + \epsilon_i) \nonumber \\
         &=& var(u_k) +  var(\epsilon_i) + 2cov(u_k,\epsilon_i) \nonumber \\
	 &=& \sigma_u^2 +  \sigma^2 + 0 \nonumber 
\end{eqnarray}
 
Further we can look at the covariance of observations 
\begin{itemize}
\item $i$ and $i'$ within group $k$
\begin{equation}
cov(y_i, y_{i'})  =  cov(u_k + \epsilon_i, u_k + \epsilon_{i'}) = \sigma_u^2 \nonumber 
\end{equation}
\item $i$ and $i'$ from different groups $k$ and $k'$ 
\begin{equation}
cov(y_i, y_{i'})  =  cov(u_k + \epsilon_i, u_{k'} + \epsilon_{i'}) = 0 \nonumber 
\end{equation}
\end{itemize}



}


\frame{
\frametitle{Random effect model and variance partition} 

\begin{block}{Variability between and within groups}{
Intra-class correlation coefficient $\rho$
\begin{equation}
\rho = cor(y_i, y_{i'})  =  \frac{cov(y_i, y_{i'}) }{\sqrt{var(y_i) var(y_{i'})}} = \frac{\sigma_u^2}{\sigma_u^2 +  \sigma^2} \nonumber 
\end{equation}
}
\end{block}

Interpretation:
\begin{itemize}
\item Intra-class correlation coefficient $\rho$ is the correlation between two observations $i$ and $i'$ in the same group.
\item It is the ratio of between-group variance $\sigma_u^2$ over the total variance.
\item If $\rho \rightarrow 0$ there is little variation explained by the grouping and we might consider a model without the random effect.
\item Any restrictions?
\end{itemize}

}


\frame{
\frametitle{Variance partition in \texttt{R}} 
\texttt{summary(RandomIntercept)} \\

 \includegraphics[scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/varRE}
\begin{equation}
\rho  = \frac{\sigma_u^2}{\sigma_u^2 +  \sigma^2} = \frac{0.6347908^2}{0.6347908^2 + 0.5764246^2} \approx 0.54  \nonumber 
\end{equation}

Interpretation:
\begin{itemize}
\item There is substantial evidence for between-group heterogeneity.
\item More than half of the total variance can be explained by the  between-group variance.
\item It is beneficial to include the random effects on the intercept.
\end{itemize}
}





\subsection{Random intercept and random slope}



\frame{
\frametitle{Random effect model with random intercept and random slope} 

\begin{enumerate}
\item Random effects model on both, the intercept and the slope
\begin{equation}
y_i = (\alpha_0 + {\color{red}{u_k}}) + (\beta + {\color{red}{w_k}}) x_i  + \epsilon_i \nonumber 
\end{equation}
\end{enumerate}

\begin{itemize}
\item There are three distinct error terms
\begin{enumerate}
\item Group-specific error of the intercept
\begin{equation}
u_k \sim N(0,\sigma_u^2) \nonumber 
\end{equation}
\item Group-specific error of the regression slope
\begin{equation}
w_k \sim N(0,\sigma_w^2) \nonumber 
\end{equation}
\item Individual-specific error
\begin{equation}
\epsilon_i \sim N(0,\sigma^2) \nonumber 
\end{equation}
\end{enumerate}
\item Note that $u_k$ and $w_k$ are correlated and independent of $\epsilon_i$.
\end{itemize}



}



\frame{
\frametitle{Random effect model with random intercept and random slope} 

\underline{Assumptions}
\begin{itemize}
\item Each group has a different intercept ($\alpha_k = \alpha_0 + u_k$) and a different regression slope ($\beta_k = \beta+w_k$).
\item We allow for correlation between $\alpha_k$ and $\beta_k$.
\item Both, $\alpha_k \sim N(\alpha_0, \sigma_u^2)$ and $\beta_k \sim N(\beta, \sigma_w^2)$  have a common distribution which is estimated from \textbf{all observations}, and not just from the observations in a given group as in fixed effects.
\item We pool information across groups.
\end{itemize}
\underline{Consequences}
\begin{itemize}
\item  Including a random slope can be interpreted as creating an interaction between the group and the strength of association.
\item  We only have three additional parameters in the model: \\ 
$\sigma_u^2, \sigma_w^2$ and $cor(\sigma_u, \sigma_w)$.
\end{itemize}


}




\frame{
\frametitle{R: Random intercept and slope using \texttt{lme}} 

\texttt{RandomSlope = lme( chol $\sim$ age, random = $\sim$ 1+age $\mid$ doctor, data = data.chol) \\
summary(RandomSlope)}

\vspace{0.2cm}
\centering \includegraphics[scale=0.38]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/summaryRS}



}

\frame{
\frametitle{R: Random intercept and slope using \texttt{lmer}} 

\texttt{RandomSlopePredictions = fitted(RandomSlope)}

\vspace{0.2cm}
\centering \includegraphics[scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-ggRandomSlope}



}


\subsection{Variables on individual level and group level}

\frame{
\frametitle{Variables on individual level and group level} 

When considering variables or predictors we need to distinguish:
\begin{itemize}
\item Individual-level variables
\item Group-level variables, that are the same for all observations in a group
\end{itemize}

GP example:\\
\begin{itemize}
\item Individual-level variables: Age and sex of patient
\item Group-level variables: Age of doctor
\end{itemize}

\centering \includegraphics[scale=0.44]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-head}


}


\frame{
\frametitle{Variables on individual level and group level} 

\begin{equation}
y_i = (\alpha_0 +u_k) + (\beta + w_k) x_i  + {\color{red}{\gamma x_g}} +\epsilon_i \nonumber 
\end{equation}

Example: GP data \\
\texttt{RandomCov = lme( chol $\sim$ age + agedoc, random = $\sim$ 1+age $\mid$  doctor, data = data.chol)\\
summary(RandomCov)}

\vspace{0.2cm}
\centering \includegraphics[scale=0.45]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/randomCov2}


}





\section{Model comparison and generalisation}

\frame{
\frametitle{Model comparison} 

\begin{itemize}
\item Likelihood-ratio test for nested models: \\
\begin{itemize}
\item Models must have the same fixed effects. Does not work with group-level covariates.
\item Model with smaller - log likelihood is better (better model fit).
\end{itemize}
\item Akaike information criterion (AIC) \\
\begin{itemize}
\item Model with the smaller AIC is better (less information loss).
\end{itemize}
\end{itemize}

GP example:\\
\begin{itemize}
\item[$\diamond$] Model A (Random intercept) \\
\texttt{modelA =  lme( chol $\sim$ age, random = $\sim$1 $\mid$ doctor, data = data.chol)}
\item[$\diamond$] Model B (Random intercept and slope) \\
\texttt{modelB =  lme( chol $\sim$ age, random = $\sim$ 1+age $\mid$  doctor, data = data.chol) }
\item[$\diamond$] Model C (Random intercept and slope and group covariate) \\
\texttt{modelC =  lme( chol $\sim$ age + agedoc, random = $\sim$ 1+age $\mid$  doctor, data = data.chol) }
\end{itemize}


}


\frame{
\frametitle{Model comparison} 

\begin{itemize}
\item Likelihood-ratio test for nested models \\ (Model A is nested in Model B) \\
\texttt{anova(modelA,modelB)} \\
\vspace{0.2cm}
\includegraphics[scale=0.35]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/anova1}



\item AIC for non-nested models \\
\texttt{anova(modelB,modelC)}
\vspace{0.2cm}
\includegraphics[scale=0.35]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/anova2}

\end{itemize}


}


\frame{
\frametitle{Generalised linear mixed models} 




 
\begin{itemize}
\item Generalised Linear Mixed models (GLMM) can be used to adapt linear mixed models to outcomes that do not follow a Normal distribution.
\item The package \texttt{lme4} includes the function \texttt{glmer} that can fit GLMMs.
\end{itemize}


\begin{block}{}{
\texttt{glmer(formula,  family = gaussian)}
}
\end{block}

Formula:
\begin{itemize}
\item \texttt{y$\sim$x} to specify outcome and predictors
\item \texttt{+ (1 $\mid$ factor)} add random intercept depending on factor
\item \texttt{+ x + (x $\mid$ factor)} add random slope depending on factor
\end{itemize}

}









\frame{
\frametitle{Take away: Fixed and random effect} 

 
\begin{itemize}
\item Fixed effect models can account for group structure but many parameters need to be estimated and no information is shared between groups.
\item Random effect models treat group-specific parameters as random variables.
\item Instead of estimating one parameter for each group, random effect models only estimate the distribution parameter of the random variable.
\item Thus, they pool information across groups.
\item The intra-class coefficient gives a measure of how relevant the group structure is.
\item Implementation in \texttt{R}: \texttt{lme()} function in the  \texttt{nlme} package.
\item Models including both, fixed and random effects, are often called linear mixed models.
\end{itemize}




}









\end{document}














\frame{
\frametitle{R: Group-specific intercept and slope in lm()} 

\begin{enumerate}
\item[3.] Group-specific intercept and slope
\begin{equation}
y_i = {\color{red}{\alpha_k + \beta_k}} x_i  + \epsilon_i \nonumber 
\end{equation}
\end{enumerate}

\begin{itemize}
\item  Add the group variable as categorical covariate and as an interaction with the predictor of interest.
\item \texttt{lm(chol $\sim$ age+as.factor(doctor) + age $:$ as.factor(doctor),  data=data.chol)}
\end{itemize}

}

\frame{
\frametitle{R: Group-specific intercept and slope in lm()} 

\centering \includegraphics[scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-ggia}

}




\frame{
\frametitle{Random effects in \texttt{R}} 

\begin{block}{Restricted Maximum Likelihood (REML)}{
is implemented in the R package \texttt{lme4}, here focus on the \texttt{lmer} function:  \\
\texttt{lmer(formula, data = NULL, REML = TRUE)}
}
\end{block}


Formula:
\begin{itemize}
\item \texttt{y$\sim$x} to specify outcome and predictors
\item \texttt{+ (1 $\mid$ factor)} add random intercept depending on factor
\end{itemize}

\url{https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf}

}

\frame{
\frametitle{R: Random effects using \texttt{lmer}} 

\texttt{RandomIntercept = lmer( chol ~ age +  (1 | doctor), data = data.chol ) \\
summary(RandomIntercept)}

\centering \includegraphics[scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/summaryRI}



}

\frame{
\frametitle{R: Random effects using \texttt{lmer}} 

\texttt{RandomInterceptPredictions = fitted(RandomIntercept)}

\centering \includegraphics[scale=0.4]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/gp-ggRandomIntercept}



}





