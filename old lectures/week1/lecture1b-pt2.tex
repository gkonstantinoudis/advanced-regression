\documentclass{beamer}

\usetheme{Montpellier}
%\usetheme{CambridgeUS}


\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}


%\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[german]{babel}
\usepackage{booktabs,bm, color, enumerate, hyperref, pgf, url, soul, tikz}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

%\usepackage{hyperref}

\usepackage{appendixnumberbeamer}




\newcommand{\nc}{\newcommand}
\nc{\pr}{\ensuremath{\mathbb{P}}}
\nc{\logit}{\rm{logit}}

\bibliographystyle{apalike}



%\input{abbreviations}



\setbeamertemplate{blocks}[rounded][shadow=true]
%\usepackage{appendixnumberbeamer}



%\subject{2015}




\title{Advanced Regression: 1b Linear and generalised linear models (Part II)}



\author{Garyfallos Konstantinoudis}
\institute{Epidemiology and Biostatistics, Imperial College London}




\date{21st February 2023}


\setlength{\unitlength}{0.9cm}
\linethickness{1.1pt}








\begin{document}


\frame{
\titlepage
}



\frame{
\tableofcontents
}



\section{Generalised linear model}

\subsection{Basic definition}


\frame{
\frametitle{Generalised linear model (GLM)} 


\begin{itemize}
\item Linear models can only model a quantitative outcome.
\item Quantitative outcomes are defined as a real number, taking possible values from $-\inf$ to $+\inf$.
\item Many important data types can by definition not be modelled using a linear model:
\begin{itemize}
\item Dichotomous or binary $\rightarrow$ only takes two values, 0 or 1
\item Counts $\rightarrow$ only positive integers (0,1,2,3,...)
\end{itemize}
\end{itemize}


\begin{block}{Generalised linear model (GLM)}{
\begin{itemize}
\item  Flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.
\end{itemize}
}
\end{block}




}


\frame{
\frametitle{Binary outcome and logistic regression} 

Example: Case-control study
 \begin{equation}
y_i = \begin{cases}
1 \text{   if subject $i$ is a case}\\
0 \text{   if subject $i$ is a control}
\end{cases}\nonumber
\end{equation} 

\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/binom.png}


}


\frame{
\frametitle{Binary outcome and logistic regression} 

\begin{equation}
y = \underbrace{\alpha + \beta x}_{\text{Linear predictor}} + \epsilon \nonumber
\end{equation} 
\begin{itemize}
\item  Linear predictor $\eta=\alpha + \beta x$ is defined from $-\inf$ to $+\inf$.
\item But $y$ can only take values $0$ or $1$ $\rightarrow$ The linear regression fit will not match the data well.
\end{itemize}
\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/binom_lm.png}

%\begin{columns}
%\begin{column}{0.5\textwidth} 
\vspace{-0.1cm}
%\end{column}
%\begin{column}{0.5\textwidth}
%\end{column}
%\end{columns}




}


\frame{
\frametitle{} 


\begin{block}{1. Key idea:}{
\begin{itemize}
\item  Instead of modelling the outcome ($y=0$ or $y=1$) directly, logistic regression models the probability for $y=1$ denotes as 
\begin{equation}
P(y=1\mid x)  \nonumber
\end{equation}
\end{itemize}
}
\end{block}

Notes on probabilities for binary data:
\begin{itemize}
\item  Probabilities can take values from 0 to 1
\item  Probabilities are symmetric
\begin{equation}
P(y=1\mid x) = 1- P(y=0\mid x) \nonumber
\end{equation} 
\end{itemize}


}




\frame{
\frametitle{Logistic function} 


\begin{block}{2. Key idea:}{
\begin{itemize}
\item  Transform the linear predictor $\eta= \alpha + \beta x_{i}$ (quantitative, can take values from $-\inf$ to $-\inf$) to lie in the Interval [0,1], which is valid for probabilities.
\end{itemize}
}
\end{block}
\begin{itemize}
\item This can be achieved using the logit function:
\end{itemize}
\begin{equation}
\logit(p) = \log (p/(1-p)) \nonumber
\end{equation} 
\vspace{-0.1cm}
\centering \includegraphics [scale=0.24]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/logit-fn}

}


\frame{
\frametitle{Logistic regression} 

\begin{block}{Logistic regression}{

\begin{equation}
\logit (P(y=1\mid x)) = \log(P(y=1\mid x)/(1-P(y=1\mid x))) = \alpha + \beta x \nonumber
\end{equation} 
}
\end{block}

\begin{itemize}
\item \textbf{Interpretation}: The regression coefficient $\beta$ in logistic regression represents the  \textcolor{red}{log odds ratio} between $y=0$ and $y=1$.
\item \textbf{Estimation}: Maximum likelihood.
\end{itemize}

}




\subsection{Technical details on exponential families and GLMs}

\frame{
\frametitle{Technical details} 
\begin{itemize}
\item Many important outcome types can be accommodated by GLMs.
\item Each of these distributions has a \textcolor{red}{location parameter}, e.g. $\mu$ for the Gaussian, $p$ for the Bernoulli and Binomial.
\item  The natural link function between the location parameter and the linear predictor can be derived from the mathematical form of the distribution.
\end{itemize}
\begin{table}[htdp]\begin{center}
\begin{tabular}{r|c|c|c}
Response & Distribution & $E(y)$ & Link $(g)$\\
\hline
Continuous & Gaussian & $\mu$ & identity   \\
Dichotomous & Bernoulli & $p$ & $\logit$  \\
Counts & Binomial & $p$ & $\logit$  \\
Counts & Poisson & $\lambda$ & log\\
\end{tabular}
\end{center}
\end{table}


}




\frame{
\frametitle{Technical details: GLM} 

The GLM consists of three elements:
\begin{enumerate}
\item A probability distribution from the \textbf{exponential family}. \\
Note: Only distributions that can be formulated as an exponential family can be modelled as GLM.
\item A linear predictor $\eta = x\beta$.
\item A link function $g$ such that $E(y) = \mu= g^{âˆ’1}(\eta)$.
\end{enumerate}

}





\frame{
\frametitle{Technical details: Exponential families} 

An exponential family is a set of probability distributions of the following form
\begin{equation}
f_x(x\mid \theta) = h(x)  \exp\{\eta(\theta) \times T(x) - A(\theta)\} \nonumber
\end{equation}
where
\begin{itemize}
\item[$\diamond$] $\theta$ is our parameter of interest
\item[$\diamond$] $T(x)$ is a sufficient statistic.
\item[$\diamond$] $\eta(\theta)$ is the natural parameter or link function.
\end{itemize}


}





\frame{
\frametitle{Gaussian distribution as exponential distribution} 

Gaussian distribution with unknown $\mu$, but known $\sigma$
\begin{eqnarray}
f_{\sigma}(x \mid \mu) & = & \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{ -\frac{(x-\mu)^2}{2\sigma^2} \}   \nonumber \\
& = & \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{ -\frac{x^2}{2\sigma^2} + \frac{x\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2} \}   \nonumber
\end{eqnarray}

\begin{itemize}
\item $ \theta = \mu $ 
\item $ h(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{ -\frac{x^2}{2\sigma^2} \} $
\item $ T(x) =  \frac{x}{\sigma}  $
\item $ \eta(\mu) = \frac{\mu}{\sigma}  $
\item $ A(\mu) = \frac{\mu^2}{2\sigma^2}$
\end{itemize}
}


\subsection{Logistic regression and binary outcomes}

\frame{
\frametitle{Logistic regression and binary outcomes} 

Binomial distribution with known number of trials $n$, but unknown probability $p$
\begin{eqnarray}
f(x \mid p) & = & {n\choose x}  p^x (1-p)^{n-x} \nonumber \\
& = &   {n\choose x} \exp\{ x \log(\frac{p}{1-p}) + n \log(1-p) \} \nonumber
\end{eqnarray}


\begin{itemize}
\item $ \theta = p $ 
\item $ h(x) = {n\choose x} $
\item $ T(x) = x $
\item $ \eta(p)  = \log(\frac{p}{1-p}) $
\item $ A(p) = -  n \log(1-p) $
\end{itemize}

}


\frame{
\frametitle{Logistic regression and binary outcomes} 

Formulate model: Three elements
\begin{enumerate}
\item Error distribution for response variable
\item Linear predictor
\item Link function
\end{enumerate}

The three elements of the logistic regression model are
\begin{enumerate}
\item The Bernoulli probability distribution modelling the data:  $\pr( y_i= 1 \mid x_i ) = p_i$
\item  The linear predictor: $\alpha + \sum_{j=1}^p \beta_j x_{ij}$
\item  The link function $g$ associating the mean of $y$, $\pr( y_i= 1 \mid x_i )$ to the linear predictor: here the link is the \textcolor{red}{logistic link}
 as we set $g(\pr( y_i= 1 \mid x_i )=\logit(p_i) = \beta_0 + \sum_{j=1}^p
\beta_j x_{ij}$
\end{enumerate}




}



\subsection{Generalised linear models in R}


\frame{
\frametitle{glm(): in R} 
\begin{itemize}
\item GLMs can be called in R just as linear models. \\
\texttt{glm(y$\_$binary $\sim$ age+sex+bmi+map+ltg, data = x, family=binomial)}
\end{itemize}

\centering \includegraphics [scale=0.42]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/glm_out}


}


\frame{
\frametitle{glm(): in R} 
\begin{itemize}
\item Different types of exponential families can be called using the \texttt{family} option:
\begin{itemize}
\item[$\diamond$]      binomial(link = `logit')
\item[$\diamond$]     gaussian(link = `identity')
\item[$\diamond$]      Gamma(link = `inverse')
\item[$\diamond$]      inverse.gaussian(link = `$1/\mu^2$')
\item[$\diamond$]      poisson(link = `log')
\end{itemize}
\item There are similar return values as for the lm function:
\begin{itemize}
\item[$\diamond$] coefficients
\item[$\diamond$] residuals
\item[$\diamond$] fitted.values
\item[$\diamond$] linear.predictors: the linear fit on link scale
\end{itemize}
\end{itemize}


}




\frame{
\frametitle{Making predictions} 

\begin{enumerate}
\item Train the prediction rule. \\
\texttt{glm$\_$predict = glm(ybin$\_$train $\sim$ glu, data = x$\_$train, family=binomial)}
\item Derive predictions on the linear scale for the new data \texttt{x$\_$tnew}. \\
\texttt{eta = predict.glm(glm$\_$predict,x$\_$new)}
\item Using the inverse logit transform to probabilities. \\
\texttt{p$\_$pred = (exp(eta)/(exp(eta)+1))}
\end{enumerate}
\vspace{-0.5cm}
\centering \includegraphics [scale=0.50]{E:/Postdoc Imperial/Lectures/2023_AdvancedRegression/AdvancedRegression2022-2023/lectures/graphs/histx_exp.png}

}








\frame{
\frametitle{Take away: Generalised linear models} 

The model formulation in GLMs consists of three elements:
\begin{enumerate}
\item Error distribution for response variable
\item Linear predictor
\item Link function
\end{enumerate}

\vspace{0.2cm}



Most common data types can be modelled using GLMs
\begin{itemize}
\item Continuous $\rightarrow$ Gaussian distribution
\item Dichotomous or binary $\rightarrow$ Bernoulli distribution
\item Counts $\rightarrow$ Poisson or Binomial (with known number of trials) distribution
\end{itemize}

}








\end{document}




