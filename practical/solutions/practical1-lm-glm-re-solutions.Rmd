---
title: "Practical 1: Linear and logistic regression, random and fixed effect"
author: "Garyfallos Konstantinoudis"
date: "Spring Term 2024"
output: pdf_document
highlight: tango
---


## Part 1: Analysing type 2 diabetes progression using linear regression


Type 2 diabetes is a long-term metabolic disorder that is characterized by high blood sugar, insulin resistance, and relative lack of insulin. The number of people diagnosed with diabetes in the UK has more than doubled in the last twenty years. According to diabetes.org.uk/ figures show that there are now almost 3.7 million people living with a diagnosis of the condition in the UK, an increase of 1.9 million since 1998.

In this practical we consider an observational study that measures progression of type 2 diabetes (quantitative score) as outcome and several clinical parameters like age. sex, and bmi, but also common risk factors like  map: blood pressure, tc: total cholesterol, ldl: low-density lipoprotein, hdl: high-density lipoprotein, tch: total cholesterol,  ltg: triglycerides, and glu: glucose. The dataset includes n=442 cases. It is available in the lars package, which is easy to install using for example the install.packages("lars") command.

As a first step we load the data and assign x as a data.frame of the predictors and y as the quantitative score of type 2 diabetes progression.progression.
```{r message=FALSE, warning=FALSE}
library(lars)
data(diabetes)
x = as.data.frame.matrix(diabetes$x)
y = diabetes$y
```

From the literature we know that the following 6 predictors are important for type 2 diabetes progression: sex, age, bmi, glu, map and ltg. We consider these 6 predictors as model 1.



Question 1.1
Look at the correlation structure between those 6 predictors and discuss the implications. Use the function corrplot() in the corrplot package to visualise the correlation structure.

*Reply: The command cor() computes the correlation matrix between the indicated variables.*
```{r message=FALSE, warning=FALSE, fig.height=3.5}
library(corrplot)
x_cor=x[,c("sex","age","bmi","glu","map","ltg")]
cor(x_cor)
corrplot(cor(x_cor))
```

*The correlation between the predictors is low to moderate. The strongest correlation is observed between ltg, map, glu,  and bmi.*

Question 1.2
Does the outcome disease progression follow a Normal-distribution? Look at general summary statistics of y, plot a histogram and a q-q plot against the Normal-distribution.

*Reply: The outcome is certainly not perfectly Normal-distributed, but for now this assumption is fine.*
```{r message=FALSE, warning=FALSE, fig.height=3}
par(mfrow=c(1,2))
hist(y,breaks=50)
qqnorm(y)
qqline(y,col="red")
par(mfrow=c(1,1))
```

Question 1.3
Fit a linear model including the predictors of model 1 (sex, age, bmi, glu, map and ltg) using the lm() function and discuss the summary of the model.

*Reply: bmi has the strongest impact on disease progression with a very small p-value, followed by ltg and map. There is a minimal sex difference. An increase of unit in ltg increases the disease progression score by 540.784. Overall there is a good model fit with an adjusted R-squared of 0.4814. *
```{r message=FALSE, warning=FALSE}
lm1 = lm(y~sex+age+bmi+glu+map+ltg, data=x)
summary(lm1)
```


Question 1.4
Perform model diagnostics and outlier detection of model 1. Do you think this is a good model fit? Justify your answers.

*Reply: All of the diagnostic plots look fine.*
```{r message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
plot(lm1,which=1)
plot(lm1,which=3)
plot(lm1,which=2)
plot(lm1,which=5)
par(mfrow=c(1,1))
```



Question 1.5
For model 1, compute the OLS regression coefficient estimate using matrix multiplication $\hat{\beta}_{OLS}=(x^t x)^{-1} x^t y$. Use the solve() function to invert a matrix. R distinguishes between scalar multiplication ($\ast$) and matrix multiplication (%$\ast$%). Make sure to use the matrix multiplication (%$\ast$%) for this task and ensure that your matrices have the correct dimensions.  Add an intercept by including a row of ones like for example this.
```{r message=FALSE, warning=FALSE}
x1 =  cbind(rep(1,nrow(diabetes$x)), x$sex, x$age, x$bmi, x$glu, x$map, x$ltg)
```
*Reply:*
```{r message=FALSE, warning=FALSE}
beta_ols = solve(t(x1) %*% x1) %*% t(x1) %*% y
```


Question 1.6
Compute the regression coefficient estimate using the sample covariance based estimate, defined as $\hat{\beta}_{COV}=cov(x)^{-1} cov(xy)$.  Use the solve() function to invert the covariance matrix cov(x) of dimension 6 x 6 and compute this estimate without the intercept using
```{r message=FALSE, warning=FALSE}
x11 =  cbind(x$sex, x$age, x$bmi, x$glu, x$map, x$ltg)
```
As an additional hint, please use the solve() function to invert the covariance matrix cov(x) of dimension 6 x 6.

*Reply:*
```{r message=FALSE, warning=FALSE}
beta_cov = solve(cov(x11)) %*% cov(x11,y)
```

Question 1.7
Compare the 3 regression coefficient estimates from questions 1.3, 1.5 and 1.6.

*Reply: All three estimators give the same answer for the regression coefficients.*
```{r message=FALSE, warning=FALSE}
compare_beta = matrix(NaN, ncol = 3, nrow = 7)
colnames(compare_beta)=c("lm","ols","cov")
compare_beta[,1] = lm1$coefficients
compare_beta[,2] = beta_ols
compare_beta[2:7,3] = beta_cov
compare_beta
```



Question 1.8
Fit model 2 that only includes glucose and compare how it differs from the  multivariable model 1.

*Reply: The effect of glucose substantially attenuates after conditioning on other covariates. In an univariable model the beta is 619, while in the multivariable model the effect is 69 and not significant. This might suggest that the disease progression does not depend on glu but on other correlated risk factors. Also the adjusted R-squared (0.1444) is much smaller than in the multivariable model. *
```{r message=FALSE, warning=FALSE}
lm2=lm(y~glu, data = x)
summary(lm2)
```


\pagebreak


## Part 2: Predict type 2 diabetes progression using linear regression


Assume we only observed the first 300 cases and use these cases as training data.
```{r message=FALSE, warning=FALSE}
x_train = data.frame(x[1:300,])
y_train = y[1:300]
```
Now we can consider the remaining 142 cases as new data points for whom we want to predict disease progression.
```{r message=FALSE, warning=FALSE}
x_new = data.frame(x[301:442,])
y_new = y[301:442]
```



Question 2.1
Use the linear model 1 to predict the disease progression for the 142 cases with predictor information stored in x_new.

*Reply: *
```{r message=FALSE, warning=FALSE}
lm1_train = lm(y_train~sex+age+bmi+glu+map+ltg, data=x_train)
y_predict1 = predict.lm(lm1_train,x_new)
```

Question 2.2
Evaluate the error of your prediction based on linear model 1 by computing the squared difference between the predicted progression and the actual observed progression saved in y_new. Plot a histogram of the squared difference and compute the mean and median.

*Reply: *
```{r message=FALSE, warning=FALSE}
sq_diff1 = (y_predict1 - y_new)^2
hist(sq_diff1, main="")
summary(sq_diff1)
```



Question 2.3
Repeat the steps 2.1 and 2.2  using the univariable linear model 2 including only glucose. Contrast the prediction error of linear model 2 with the prediction error of linear model 1.

*Reply: The prediction using model 2 (glu) has a much larger mean squared error (MSE) than model 1 (sex, age, bmi, glu, map and ltg). *
```{r message=FALSE, warning=FALSE}
lm2_train = lm(y_train~glu, data=x_train)
y_predict2 = predict.lm(lm2_train,x_new)
sq_diff2 = (y_predict2 - y_new)^2
hist(sq_diff2, main="")
summary(sq_diff2)
```


Question 2.4
Is it good practise to evaluate the prediction performance on a single training data? How appropriate is the split to take the first 300 cases?

*Reply: No, ideally one would repeat the process (split the data, learn the prediction rule and evaluate) many times to eliminate the randomness of the data split. When just doing this process once we only might capture a prediction rule that is specific for the first 300 cases, but does not generalise to other data-sets. The main aim of building a prediction rule is that it generalises to new data. *

\pagebreak



## Part 3: Distinguishing between severe and mild cases of type 2 diabetes using logistic regression


Doctors are particularly concerned with type 2 diabetes cases that have a bad disease progression, in particular cases that have a disease progression score larger than 200. Binarise your outcome like this:
```{r message=FALSE, warning=FALSE}
y_binary = as.numeric(y>200)
```


Question 3.1
Fit a generalised linear model using the glm() function that can distinguish between bad disease progression and normal progression. Use the 6 predictors as considered in model 1.
Look at the summary of the glm output and interpret the findings.

*Reply: Similar like in the linear model we find the strongest effects for bmi and ltg, and also for map.
The beta coefficients represent log odds, so the odds-ratio of glu is equal to exp(13). The effect of glu is not significant. *
```{r message=FALSE, warning=FALSE}
glm1 = glm(y_binary~sex+age+bmi+glu+map+ltg, data = x, family=binomial)
summary(glm1)
```


Question 3.2
Consider now model 2 including only glucose. Fit a glm and see if glucose can distinguish between bad disease progression and normal progression.

*Reply: When not considering the other covariates the effect of glu becomes highly significant also for the binary trait. But again the model fit (look here at AIC (Lecture 2a)) is not as good as for model 1. The AIC of model 1 is much smaller than the AIC of model 2, thus model 1 provides the better overall fit. *
```{r message=FALSE, warning=FALSE}
glm2 = glm(y_binary~glu, data = x, family=binomial)
summary(glm2)
```


Question 3.3
Look again at the training data (x_train and ybin_train) based on the first 300 cases, where
```{r message=FALSE, warning=FALSE}
ybin_train = y_binary[1:300]
```
Build a prediction rule based on model 1 using the training data (x_train and ybin_train) using the glm function. In a second step predict which of the new samples (using x_new as predictor matrix) are at high risk for having a bad diagnosis. How many of the 142 new observations have a probability larger than 0.5 to have bad progression?

PS Use the inverse logit function $(logit^{-1} (eta) = exp(eta)/(exp(eta) + 1))$ to transform the linear predictor $(eta = x \beta)$ back to a probability which ranges between 0 and 1.

*Reply: First we learn the prediction rule and we predict the linear predictor eta for the new observations. Note that eta is on the linear and continuous scale.*
```{r message=FALSE, warning=FALSE}
glm_predict = glm(ybin_train~sex+age+bmi+glu+map+ltg, data = x_train, family=binomial)
eta = predict.glm(glm_predict,x_new)
summary(eta)
```
*In order to transform the linear predictor to probabilities we use the inverse logit link. There are 39 cases that are predicted to have a bad disease progression. *
```{r message=FALSE, warning=FALSE}
p_predict = (exp(eta)/(exp(eta)+1))
summary(p_predict)
sum(p_predict>0.5)
```

\pagebreak



## Part 4 (Optional): Which risk factors are important for type 2 diabetes progression?

Look again at the complete dataset including all n=442 cases and all 10 predictors. How would you perform variable selection to decide which variables are important for disease progression in type 2 diabetes?

*Reply: First we can fit a linear regression including all predictors. *
```{r message=FALSE, warning=FALSE}
lm_all=lm(y~as.matrix(x))
summary(lm_all)
```
*The full model includes many covariates that do not have a significant effect, eg. age or hdl. Still for the interpretation we would prefer to have our model adjusted for age. Some clincal covariates might be better left in a model so that they do not act as a confounder. With respect to variable selection, backward or forward selection are often used, but they are not recommended as these approaches depend on the order how variables are included. In case you want to test a very specific hypothesis, for example if glu should be included or not it is possible to use an anova test for linear models. *

```{r message=FALSE, warning=FALSE}
lm_noglu=lm(y~as.matrix(x[,-10]))
anova(lm_noglu,lm_all)
```
*The above test says that there is no improvement in the model fit when including glu. This type of comparison of nested models is only valid to test very specific hypothesis, and not for variable selection in general. In lectures 2a and 2b we will learn more about variable selection and in lecture 3c we cover regularised regression approaches like lasso and elastic net that can perform variable selection and in lectures 4b we cover machine learning approaches to define variable importance. *


\pagebreak



## Part 5: Linear mixed model: Exam scores from London

This section considers exam scores of 3,935 students from 65 schools in Inner London. In particular, we want to find out how the final exam score can be predicted by reading abilities as measured in the London reading (LR) test. Please adjust the path to the dataset according to your computational setup.

```{r message=FALSE, warning=FALSE}
load("../../data/exam.London")
```

Additional covariates of the data are:

- school: 	School ID - a factor
- schgend: 	School gender - a factor. Levels are ‘mixed’, ‘boys’, and ‘girls’
- schavg: 	School average of intake score
- vr: 	Student level Verbal Reasoning (VR) score band at intake - ‘bottom 25%’, ‘mid 50%’, and ‘top 25%’
- intake: 	Band of  student’s intake score - ‘bottom 25%’, ‘mid 50%’ and ‘top 25%’
- sex: 	Sex of the student - levels are ‘F’ and ‘M’
- type: 	School type - levels are ‘Mxd’ and ‘Sngl’
- student: 	Student id (within school) - a factor


Question 5.1

Fit a linear model to test if there is a linear relationship between reading ability and the final exam score and plot a scatterplot of exam score against reading ability.

```{r message=FALSE, warning=FALSE, fig.height =3.5}
exam.lm = lm(normexam~standLRT, data=exam)
summary(exam.lm)
plot(exam$standLRT,exam$normexam, xlab="Reading (LR) test", ylab="Final exam score")
abline(exam.lm, lwd=2, col="red")
```

*Reply: There is a linear relationship between reading ability (standardised London reading test) and the final exam score. An increase of 1 in the reading score, increases the final exam score by 0.596474.*


Question 5.2

Are there any potential issues with the standard linear model?

*Reply: Yes, this linear model treats all observations as independent and disregards the potential group structure as induced by the school in which the students are studying.*

Question 5.3

Fit a fixed effect model accounting for the effect of schools using the lm() function where you add school (as.factor()) as covariate.
What is the interpretation of the model and how many additional parameters do we need to estimate?

```{r message=FALSE, warning=FALSE}
exam.fe = lm(normexam~standLRT+as.factor(school), data=exam)
coef(exam.fe)
```

*Reply: Also the fixed effects model finds a linear relationship where the regression coefficient equals 0.560181196. By introducing the factor school as covariate we need to estimate additional to the intercept and the beta regression coefficient another 64 regression parameter, one for each school minus the reference category. In total these are 66 parameters to estimate in a fixed effects model.*

Question 5.4

Now use the function in the lme function in the
```{r message=FALSE, warning=FALSE}
library(nlme)
```
package to estimate a random effects model with a random intercept depending on the school. What is the interpretation of the fixed effect? How many parameters do we need to estimate compared to the fixed effects model?

```{r message=FALSE, warning=FALSE}
RandomIntercept = lme(normexam~standLRT,  random = ~1|school, data=exam)
summary(RandomIntercept)
```
*Reply: There is almost no change in the interpretation of the random effects model, also here the reading ability has a linear relationship with the exam score (beta=0.5641318). We need to estimate two additional parameters, StdDev(Intercept), StdDev(Residual), so in total there are 4 parameters, 62 parameters less than in the fixed effects model.*

Question 5.5

What is the intra-class correlation coefficient for this model (lecture 1c, slide 38-40) and how do you interpret it?

```{r message=FALSE, warning=FALSE}
std.u = 0.3071927
std.e = 0.7535887
rho = std.u^2 / (std.u^2+std.e^2)
rho
```
*Reply: The intra-class correlation coefficient is 0.1424922, which is not strong, but still it should be accounted for.*


Question 5.6

Add a random slope depending on school to your model and see if the effect of the fixed effects changes.

```{r message=FALSE, warning=FALSE}
RandomSlope = lme(fixed=normexam~standLRT, random = ~ 1 + standLRT | school, data = exam)
summary(RandomSlope)
```
*Reply: No, the strength of the fixed effect of reading ability stays roughly constant with a beta equal to 0.5572344.*


Question 5.7

Which of the covariates are individual-level and which are group-level variables? Re-fit your random intercept model adding the group-level variables to the random effects model.


```{r message=FALSE, warning=FALSE}
RandomInterceptCov = lme( normexam~standLRT + schavg + schgend, random = ~ 1 | school, data = exam)
summary(RandomInterceptCov)
```
*Reply: Group-level covariates are school gender, school average of intake score and school type. We add here the covariates  school gender and  school average of intake score. We find that the  school average of intake score and the effect of being in a girls only school have a significant positive effect. The fixed effect of reading ability (beta=0.5601841) is comparable to other models.*

Question 5.8

Compare the random intercept (Q5.4) and the random intercept and slope model (Q5.6) using the likelihood ratio test and discuss which one has the better model fit.

```{r message=FALSE, warning=FALSE}
library(lmtest)
lrtest(RandomIntercept, RandomSlope)

# notice that the anova() function gives more information
anova(RandomIntercept, RandomSlope)
```
*Reply: The random slope model has a better model fit than the random intercept model in all criteria, likelihood ratio (these are nested models, so it is ok to interpret the the likelihood ratio test here), AIC and BIC.*

Question 5.9

Compare the random intercept model (Q5.4) and the one with the additional covariates (Q5.7) using the AIC and BIC (note that those two models are not nested) and discuss which one has the better model fit.

```{r message=FALSE, warning=FALSE}
anova(RandomIntercept, RandomInterceptCov)
```
*Reply: Since the two models are not nested we need to compare them using the AIC and BIC. We find that there is no conclusive improvement when adding the covariates. Using the AIC the covariate model would be better, using the BIC the intercept only model is better.*

\pagebreak





## Part 6: Linear mixed model: Survival on the Titanic

The sinking of the titanic was one of the greatest disaster in navel history. After colliding with an iceberg, the titanic sank and 1,502 out of 2,224 passengers and crew were killed. The following data set has collected information on n=1,309 of the passengers and their survival.

```{r message=FALSE, warning=FALSE}
titanic = read.csv("../../data/titanic.csv")
dim(titanic)
table(titanic$survived)
```
The dataset includes:

- survival: Survival (0 = No; 1 = Yes)
- class: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
- name: Name
- sex: Sex (1=female, 2=male)
- age: Age
- sibsp: Number of Siblings/Spouses Aboard
- parch: Number of Parents/Children Aboard
- ticket: Ticket Number
- fare: Passenger Fare
- cabin: Cabin
- embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
- boat: Lifeboat (if survived)
- body: Body number (if did not survive and body was recovered)


For more information on the data and a data challenge called 'Machine Learning from Disaster' see

https://www.kaggle.com/c/titanic

In the following we want to test if the phrase  'women and children first' was adapted for the evacuation of the titanic.

Question 6.1

Since survival is a binary outcome here, use a glm to test if age and sex had an effect on survival.
```{r message=FALSE, warning=FALSE}
glm.out = glm(as.factor(survived)~as.factor(sex)+age, family = "binomial", data = titanic)
summary(glm.out)
```
*Reply: We indeed see a worse chance of survival for men, but no effect of age.*


Question 6.2

Next step is to account for the passenger class (variable pclass) in a fixed effects model and discuss the implications and difference to the simple model.
```{r message=FALSE, warning=FALSE}
glm.out.fixed = glm(as.factor(survived)~as.factor(pclass)+as.factor(sex)+age,
 family = "binomial", data = titanic)
summary(glm.out.fixed)
```
*Reply: After accounting for passenger class, we find a significant effect of age. The beta coefficient is negative, which means that there is a better chance for survival for younger passengers. This indicates that we need to take into account the class, in order to see the true effect of age. If we do not account for the passenger class, this might act as a confounder. In the fixed effect we can also interpret the effect of the passenger class, in particular the best survival is seen for the first class, the reference category. The worst chance of survival is for passengers in the third class.*

Question 6.3

Discuss whether to include the passenger class as a fixed or random effect and fit a random effects model with a random intercept depending on passenger class using the glmer() function in the
```{r message=FALSE, warning=FALSE}
library(lme4)
```
package.

```{r message=FALSE, warning=FALSE}
lmm.intercept = glmer(as.factor(survived)~as.factor(sex)+age+(1|pclass),
  family = "binomial", data = titanic)
summary(lmm.intercept)
```
*Reply: When fitting a random effects model we do account for potential group structure induced by the passenger class, but we do not estimate the effect of each passenger class per se. There is no easy formula for the intra-class correlation coefficient as for linear outcome. We would need to call a different package (sjstats::icc and package glmmTMB to compute the generalised linear mixed model) to compute it. If we are only interested in the hypothesis 'Women and children first' we can evaluate this using the generalised mixed model. The interpretation of fixed and random effects with respect to the hypothesis is the same, both support an effect of sex and age on survival. Since class has only three categories it would be fine to use the fixed effects model (adding two additional parameters to the model). For comparison, the random intercept model has one additional parameter. In the fixed effects model we can additionally interpret the class specific survival chance.*


Question 6.4

Add a random slope depending on passenger class to your model and compare it with the random intercept only model using a likelihood test.
```{r message=FALSE, warning=FALSE}
lmm.slope = glmer(as.factor(survived)~as.factor(sex)+age+(age|pclass),
 family = "binomial", data = titanic)
summary(lmm.slope)
```
*Reply: Also the random slope and intercept model supports an effect of both sex and age.*
```{r message=FALSE, warning=FALSE}
anova(lmm.intercept, lmm.slope)
```
*Reply: When considering random effects model, the random intercept model has a better model fit.*

Question 6.5

How do you explain the difference in results after accounting for passenger class? Use a boxplot and a violin plot for age depending on passenger class to illustrate your argument.
```{r message=FALSE, warning=FALSE, fig.height =3.5}
boxplot(titanic$age~titanic$pclass)
```

*Reply:  There is a negative correlation between passenger class and age. Passengers travelling in lower classes were younger than passengers in the first class as shown in the boxplot of age grouped by passenger class.*


```{r message=FALSE, warning=FALSE, fig.height =2.5}
library(ggplot2)
p = ggplot(titanic, aes(factor(pclass), age))
p + geom_violin(na.rm = TRUE)
```

*An alternative for the boxplot is the violin plot, a mixture of a boxplot and a kernel density function. Here it is even more obvious that there were much more young children travelling in class 2 and 3. Especially young children had a good chance for survival. In order to estimate the true effect of age we need to adjust for passenger class (either in a fixed or random effects model). Otherwise the estimate would be confounded since there where few children travelling first class.*






<!--
require(knitr)
require(markdown)
require(rmarkdown)
render("practical1-lm-glm-re-solutions.Rmd")
-->


