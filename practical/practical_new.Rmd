---
title: "New Practical Material: Finding a prediction rule for heart disease"
author: "Verena Zuber, Devleena Ray, Paula Christen, Rima Mustafa"
date: "Spring Term 2021"
output: pdf_document
highlight: tango
---



## The heart.csv datasets: Clinical measurements and heart disease

The following computational task is attempting to distinguish between cases with heart disease and controls. The dataset has been downloaded from the ML repository
https://archive.ics.uci.edu/ml/datasets/heart+Disease

Please load the data as follows and take a look at the binary case-control outcome $y$ and the predictor matrix $x$.
```{r message=FALSE, warning=FALSE}
load("../../data/heart.Rdata")
x = heart.data$x
y = as.factor(heart.data$y)
table(y)
head(x)
summary(x)
```

The predictor matrix includes

- age
- sex
- chest pain type  (1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic)
- resting blood pressure
- serum cholesterol in mg/dl
- fasting blood sugar > 120 mg/dl
- resting electrocardiographic results (0: normal, 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), 2: showing probable or definite left ventricular hypertrophy)
- maximum heart rate achieved
- exercise induced angina
- oldpeak = ST depression induced by exercise relative to rest


There are several categorical variables in the predictor matrix; the following code transforms them into dummy variables.
```{r message=FALSE, warning=FALSE}
x.mat = model.matrix( ~ ., x)[,-1]
x.mat = as.matrix(x.mat)
head(x.mat)
dim(x.mat)
```


## Part 1: GLMNET and parameter tuning (Practical 3)


Please load the following R-packages.
```{r message=FALSE, warning=FALSE}
library(glmnet)
library(coefplot)
```


Question 1.1

Fit a lasso model using glmnet with a fixed regularisation parameter lambda = 0.1. How many variables are included in the model?


Question 1.2

Perform cross-validation to tune the regularisation parameter of the lasso, which measure would you use to evaluate the cross-validation?


Question 1.3

How many variables are included in the lasso model when using the regularisation parameter that minimises the cross-validation error and the largest regularisation parameter (strongest regularisation) which is within 1 standard error of the minimum?


Question 1.4

Use the $coefplot$ function in the coefplot package to visualize the lasso regression coefficients for the two lasso models with lambda selected as $lambda.min$ and $lambda.1.se$. Please interpret the findings.


Question 1.5

Fit one lasso model which does not specify the regularisation parameter and plot the object
```{r message=FALSE, warning=FALSE}
lasso.all = glmnet(x.mat,y,family="binomial",alpha=1)
plot(lasso.all, xvar='lambda', label=TRUE)
```
How do you interpret the plot? Alternatively, use the $coefpath$ function in the $coefplot$ package to visualize how the lasso regression coefficients depend on the regularisation parameter. Please interpret this plot.


## Part 2: Prediction performance of regularised regression (Practical 4)


Please load the following R-packages.
```{r message=FALSE, warning=FALSE}
library(glmnet)
library(crossval)
```


Question 2.1

To get started, fit first a glm to train a prediction rule on the data using $y$ as outcome and $x.mat$ as predictor. Derive the predicted probabilities for all 303 individuals. Repeat the exactly the same prediction using the lasso with lambda = 0.1.


Question 2.2

Perform cross-valisation to tune the regularisation parameter for L1 regularised regression (lasso) and L2 regularised regression (ridge). Set the seed to 22.


Question 2.3

Write a prediction function for the glmnet function to use for cross-validation to evaluate the predictive performance using the $crossval$ package. Make sure to leave lambda and alpha as open parameters. Please return the $confusionMatrix$ as final output of the function.


Question 2.4

Compare the standard glm model (using lambda = 0), with ridge and lasso regression using cross-validation. Set the seed to 24. Which model performs best and is there a big difference between the models?




## Part 3: Decision trees and random forests (Practical 5)


Please load the following R-packages.
```{r message=FALSE, warning=FALSE}
library(crossval)
library(randomForest)
library(tree)
```


Question 3.1

Fit a decision tree to model the outcome $y$ (heart disease 1 vs control 0) by the predictor matrix $x.mat$ and plot the decision tree. What is a key issue with decision trees?


Question 3.2

One way to counteract over-fitting is to perform cross-validation (set seed to 32) and prune the decision tree. Please plot the pruned tree and compare it with the unpruned tree. How many splits does the pruned tree include?


Question 3.3

An alternative strategy is to perform a meta-algorithm that averages over many decision trees fitted on small variations of the input data (bagging). Perform a random forest and the original bagging algorithm.


Question 3.4

Use the variable importance measures of the random forest algorithm to rank the features by their importance.


Question 3.5

Write a prediction function for the random forest algorithm to be evaluated using the $crossval$ function. Please leave the number of trees and mtry as an open parameter and return the confusion matrix as implemented in the $confusionMatrix$ function.


Question 3.5

Which algorithm has the better prediction performance, a decision tree, bagging or random forest? You can fit a decision tree by just fitting a single tree with the ntree option set to 1 and mtry to ncol(x.mat). Please perform crossvalidation with the crossval function with $K=10$ and $B=20$ and set the seed to 35.



## Part 4: The heart.csv datasets: Using the caret framework (Practical 5)


In order to evaluate a prediction rule we have so far defined out own prediction rules and used the crossval package for comparison. This is a useful approach when learning to understand how prediction algorithms and cross-validation work. Yet, this is not a principled approach to compare many algorithms.


This is why the caret package (short for Classification And REgression Training) has become very popular. It provides "a set of functions that attempt to streamline the process for creating predictive models". Please see the webpage for a detailed description

https://topepo.github.io/caret/

Here, we are going to use the caret package to compare prediction performance of various algorithms. In particular, see the following table here for an overview of available algorithms

https://topepo.github.io/caret/available-models.html

Please before loading caret, unload the crossval package in order to avoid masking (overwriting of functions with the same name).
```{r message=FALSE, warning=FALSE}
detach("package:crossval", unload=TRUE)
library(caret)
```
Split your data into training and test data to evaluate the final performance. Set your seed to 4 to replicate your findings.
```{r message=FALSE, warning=FALSE}
set.seed(4)
data = as.data.frame(cbind(y,x.mat))
index = createDataPartition(y, p = 0.8, list = FALSE)
data.train = data[index, ]
data.test = data[-index, ]
```

We are going to perform repeated cross-validation with K=10 folds and 2 repetitions (Please increase to at least 10 or more in reality). Other options would be bootstrapping and leave-one-out cross-validation.
```{r message=FALSE, warning=FALSE}
train.control = trainControl(method="repeatedcv", number=10, repeats=2)
```

Question 4.1 Fit a GLM

Use the train function in caret to fit a GLM by using method="glm" and family "binomial". Please use the training data stored in data.train only.


Question 4.2 Fit glmnet

Now, change the method from "glm" to "glmnet" to perform regularised regression.


Question 4.3 Parameter tuning

Glmnet has two tuning parameters:

- alpha (Mixing Percentage)
- lambda (Regularization Parameter)

The training function in caret performs internal cross-validation to tune these two hyperparameters. Please check which hyperparameter combination obtained the best performance in the $bestTune value of the object. What are the variables included? Check using the coef() function.


Question 4.4 Fit a support vector machine

Perform support vector machine by changing method to 'svmLinearWeights'.


Question 4.5 Fit a random forest

Perform random forest by changing the method to 'rf'. The hyperparameter is mtry. Which one has the best performance?



Question 4.6 Evaluate all methods on the test data

caret has also streamlined the prediction performance. Please evaluate all four algorithms (glm, glmnet, svm and random forest) to check which algorithm has the best prediction performance on the test data $data.test$.




## Part 5: Defining a prediction rule for transcriptomics: Using the caret framework (Practical 5)


We look back at the dataset which links gene-expression with the response of breast cancer patients to treatment (Practical 3, Q3 and Practical 4, Q1). Our aim is to build a classifier that can discriminate between 2 responder groups (pathologic complete response or minimal residual cancer burden [RCB-I] defining excellent response (coded as 0), vs moderate or extensive residual cancer burden [RCB-II/III] defining lesser response (coded as 1).

For the original publication please see https://jamanetwork.com/journals/jama/fullarticle/899864

Load the dataset including $n=414$ patients and the predictor matrix including expression of $p=22,283$ genes.
```{r message=FALSE, warning=FALSE}
load("../../data/JAMA2011_breast_cancer")
#alternatively try load("../../data/JAMA2011_breast_cancer.dms")
y = as.factor(data_bc$rcb)
table(y)
x = as.matrix(data_bc$x)
dim(x)
```

This practical exercise is using the caret package to contrast different prediction algorithms in a streamlined fashion. Please see the previous question for more details on the caret package.
```{r message=FALSE, warning=FALSE}
library(caret)
```

We are going to perform repeated cross-validation. Other options would be bootstrapping and leave-one-out cross-validation.
```{r message=FALSE, warning=FALSE}
train.control = trainControl(method="repeatedcv", number=10, repeats=2)
```


Question 5.1 Pre-filter the transcripts

Unfortunately the dataset is too large to be analysed using caret. This is why we perform variable ranking as outlined in Practical 3, question 3 based on shrinkage t-score and select only variables which have a local false discovery rate < 0.5.
Please use the shrinkt.stat in the st package to fit the shrinkage t-score and the fdrtool function in the fdrtool package to compute the shrinkage t-scores and the local false discovery rate.

How many genes remain after filtering?


Question 5.2 GLMNET

Fit the glmnet using caret by specifying as method "glmnet". What is the accuracy?


Question 5.3 SVM

Fit a support vector machine using caret "svmLinearWeights". What is the accuracy?


Question 5.4 Random forest

Fit a random forest using caret "rf". What is the accuracy? Which of the three algorithms has the best accuracy?






<!--
require(knitr)
require(markdown)
require(rmarkdown)
render("practical_new.Rmd")
-->

